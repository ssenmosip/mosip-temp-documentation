{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to MOSIP's documentation!\n\n\nGet the code\n\n\nThe source http://github.com/mosip is available on Github.  \n\n\nThe mailing list for the project is located at google groups:  \n\n\nHere lets link to a page and see how it shows up - To get started on contributing to MOSIP, read our \nContributor Guide\n and our \nCode of Conduct\n.\n\n\n\n\nPre-Registration APIs\n\n\nRegistration Processor APIs\n\n\nID Authentication APIs\n\n\nABIS APIs\n\n\nBiometric APIs\n\n\nOTP Manager API",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-mosips-documentation",
            "text": "",
            "title": "Welcome to MOSIP's documentation!"
        },
        {
            "location": "/#get-the-code",
            "text": "The source http://github.com/mosip is available on Github.    The mailing list for the project is located at google groups:    Here lets link to a page and see how it shows up - To get started on contributing to MOSIP, read our  Contributor Guide  and our  Code of Conduct .   Pre-Registration APIs  Registration Processor APIs  ID Authentication APIs  ABIS APIs  Biometric APIs  OTP Manager API",
            "title": "Get the code"
        },
        {
            "location": "/Architecture/",
            "text": "MOSIP Architecture\n\n\nThe Modular Open Source Identity Platform (MOSIP) helps Governments and other user organizations implement a digital, foundational identity system in a cost effective way. Nations can use MOSIP freely to build their own identity systems. Being modular in its architecture, MOSIP provides flexibility to countries in how they implement and configure their systems, and helps avoid vendor lock-in.\n\n\nMOSIP provides the following basic features - \n\n acquire an individual's identity data\n\n process the identity data to establish uniqueness\n\n generate a Unique Identity Number (UIN)\n\n authenticate an individual's identity where required to provide access to services such as healthcare, education, social security etc. \n\n\nThe key objectives of the platform are to - \n\n provide the basic framework to create a fully functional identity system\n\n provide the flexibility for a country to choose the features from the basic framework according to their requirements\n\n maintain the privacy, security and confidentiality of an individual's data\n\n provide a scalable and accessible solution to cater to a wide range of population (a few thousands to tens of millions)\n\n\nArchitectural Principles\n\n\nMOSIP is built on the following architecture principles\n\n\n\n\nMOSIP must not use proprietary or commercial license frameworks. Where deemed essential, such components must be encapsulated to enable their replacement if necessary (to avoid vendor lock-in)\n\n\nMOSIP must use open standards to expose it\u2019s functionality (to avoid technology lock-in)\n\n\nEach MOSIP component must be independently scalable (scale out) to meet varying load requirements\n\n\nMOSIP must use commodity computing hardware & software to build the platform\n\n\nData must be encrypted in-flight and at-rest. All requests must be authenticated and authorized. Privacy of Identity Data is an absolute must in MOSIP\n\n\nMOSIP must follow platform based approach so that all common features are abstracted as reusable components and frameworks into a common layer\n\n\nMOSIP must follow API first approach and expose the business functions as RESTful services\n\n\nMOSIP must follow the following manageability principles \u2013 Auditability & monitor ability of every event in the system, testability of every feature of the platform & easy upgrade ability of the platform\n\n\nMOSIP components must be loosely coupled so that they can be composed to build the identity solution as per the requirements of a country\n\n\nMOSIP must support i18n capability\n\n\nAll modules of MOSIP should be resilient such that the solution as a whole is fault tolerant\n\n\nThe key sub-systems of MOSIP should be designed for extensibility. For example, if an external system has to be integrated for fingerprint data, it should be easy to do so\n\n\n\n\nThe key design aspects considered for MOSIP are\n\n\nEcosystem approach\n\n\nMOSIP on its own will not be able to meet the end-to-end requirements of a country. Devices and ABIS providers are key to process an individual's data and prove uniqueness. MOSIP should be able to integrate with devices and ABIS that conform to the standards to achieve the stated goals. MOSIP should also be able to cater to a diverse set of institutions wanting to authenticate an Individual against the data stored in MOSIP.\nSo, key parameters are\n* All public/external facing interfaces of MOSIP must be standards-based for interoperability\n\n\nConfigurability\n\n\nMOSIP should be flexible for countries to configure the base platform according to their specific requirements. Some of the examples of configurability are\n\n Country should be able to choose the features required. For example, it must be possible for a country to turn off Finger Print capture\n\n Country should be able to configure the attributes of an ID Object\n* Country should be able to define the length of the UIN number\n\n\nExtensibility\n\n\nMOSIP should be flexible to extend functionality on top of the basic platform. Some of the examples of extensibility are\n\n A country should be able to introduce a new step in processing data\n\n Integrate MOSIP with other ID systems and include it as part of the MOSIP data processing flow\n\n\nModularity\n\n\nAll components in MOSIP should be modular and their features exposed via interfaces such that the implementation behind the interface can be changed without affecting other modules. Some examples of modularity are\n\n UIN generator algorithm provided by the platform can be replaced by a country with their own implementation\n\n The default demographic deduplication algorithm provided by MOSIP can be changed to a different one without impacting the process flow\n\n\nLogical Architecture\n\n\n\n\nMicroservice based architecture for all platform services\n\n\nServices must be stateless to scale out horizontally\n\n\nServices must be idempotent\n\n\nServices must have well defined interfaces\n\n\n\n\n\n\n\n\nThis will help achieve modularity, better maintainability and scalability.\n\n\n\n\nStaged Event Driven Architecture (SEDA) for processing Registration data\n\n\nEach processing step must be a stage\n\n\nStages must be connected with event bus\n\n\n\n\n\n\nProcess flow must be configured outside code\n\n\n\n\nThis will help achieve extensibility and scale out stages independently.\n\n\n\n\nThick client architecture for Registration client\n\n\nClient must run on a desktop/laptop in offline mode also\n\n\nClient must integrate with devices in a secure manner\n\n\n\n\n\n\n\n\n\n\nData Architecture\n\n\nOpen Source and Vendor Neutral\n\n\nTo handle vendor neutrality and open source, the following consideration are followed while designing the data model and the database design.\n\n\n\n\n\n\nNo business logic is applied at database level: Database will be used only to store and retrieve data. There is no business logic applied at database level other than Primary / Unique key, Not null and foreign keys. Foreign keys are applied within the same database, if a table is referenced in another database then no FK is applied. \n\n\n\n\n\n\nNo specific database features to be used: Features that are common across databases which are compliant with open source standards are applied. \n\n\n\n\n\n\nAll DDL, DML and DQL statements will follow ANSI standards\n\n\n\n\n\n\nMetadata approach to handle complex and flexible data structures\n\n\n\n\n\n\nOnly following datatypes are being used\n\n\n\n\nCharacter varying\n\n\nTimestamp\n\n\nDate\n\n\nInteger\n\n\nNumber\n\n\nBytea/blob\n\n\nBoolean\n\n\n\n\n\n\n\n\nMulti-Language\n\n\nMOSIP platform is being built for multiple countries, there is a need to support multiple languages. So as per the requirements, MOSIP will support 3 languages as configured by the country level administrator.\nMulti language support is needed for the following datasets. \n\n\n\n\nMaster Data\n\n\nID data of an individual\n\n\nTransaction comments\n\n\nLabels used in UI\n\n\nMessages and notifications\n\n\n\n\nFrom database side, the data will support \nUTF-8 Unicode character set\n to store data entered in multiple languages. \nThere will not be any in-built support to translate data at database level. Any translation or transliteration will be handled at API or UI layer.\n\n\nHigh Performance\n\n\nTo support high performance, following database design features are to be considered\n\n\n\n\nDatabase sharding is applied on uin dataset. By default, base sharding algorithm will be applied in MOSIP system. SI can define the sharding algorithm based on the deployment setup\n\n\nAll tables will have a primary key index on the primary key field. This will help in faster retrievals and joins\n\n\nAll foreign keys will have indexes defined so that it will help in faster joins\n\n\nNo referential integrity is applied on tables across databases\n\n\nPartitioning: Partitioning design to be discussed as PostgreSQL has certain limitation / different way of implementation that requires specific database features to be applied. To be discussed further to finalize the implementation of this feature.",
            "title": "Architecture"
        },
        {
            "location": "/Architecture/#mosip-architecture",
            "text": "The Modular Open Source Identity Platform (MOSIP) helps Governments and other user organizations implement a digital, foundational identity system in a cost effective way. Nations can use MOSIP freely to build their own identity systems. Being modular in its architecture, MOSIP provides flexibility to countries in how they implement and configure their systems, and helps avoid vendor lock-in.  MOSIP provides the following basic features -   acquire an individual's identity data  process the identity data to establish uniqueness  generate a Unique Identity Number (UIN)  authenticate an individual's identity where required to provide access to services such as healthcare, education, social security etc.   The key objectives of the platform are to -   provide the basic framework to create a fully functional identity system  provide the flexibility for a country to choose the features from the basic framework according to their requirements  maintain the privacy, security and confidentiality of an individual's data  provide a scalable and accessible solution to cater to a wide range of population (a few thousands to tens of millions)",
            "title": "MOSIP Architecture"
        },
        {
            "location": "/Architecture/#architectural-principles",
            "text": "MOSIP is built on the following architecture principles   MOSIP must not use proprietary or commercial license frameworks. Where deemed essential, such components must be encapsulated to enable their replacement if necessary (to avoid vendor lock-in)  MOSIP must use open standards to expose it\u2019s functionality (to avoid technology lock-in)  Each MOSIP component must be independently scalable (scale out) to meet varying load requirements  MOSIP must use commodity computing hardware & software to build the platform  Data must be encrypted in-flight and at-rest. All requests must be authenticated and authorized. Privacy of Identity Data is an absolute must in MOSIP  MOSIP must follow platform based approach so that all common features are abstracted as reusable components and frameworks into a common layer  MOSIP must follow API first approach and expose the business functions as RESTful services  MOSIP must follow the following manageability principles \u2013 Auditability & monitor ability of every event in the system, testability of every feature of the platform & easy upgrade ability of the platform  MOSIP components must be loosely coupled so that they can be composed to build the identity solution as per the requirements of a country  MOSIP must support i18n capability  All modules of MOSIP should be resilient such that the solution as a whole is fault tolerant  The key sub-systems of MOSIP should be designed for extensibility. For example, if an external system has to be integrated for fingerprint data, it should be easy to do so   The key design aspects considered for MOSIP are",
            "title": "Architectural Principles"
        },
        {
            "location": "/Architecture/#ecosystem-approach",
            "text": "MOSIP on its own will not be able to meet the end-to-end requirements of a country. Devices and ABIS providers are key to process an individual's data and prove uniqueness. MOSIP should be able to integrate with devices and ABIS that conform to the standards to achieve the stated goals. MOSIP should also be able to cater to a diverse set of institutions wanting to authenticate an Individual against the data stored in MOSIP.\nSo, key parameters are\n* All public/external facing interfaces of MOSIP must be standards-based for interoperability",
            "title": "Ecosystem approach"
        },
        {
            "location": "/Architecture/#configurability",
            "text": "MOSIP should be flexible for countries to configure the base platform according to their specific requirements. Some of the examples of configurability are  Country should be able to choose the features required. For example, it must be possible for a country to turn off Finger Print capture  Country should be able to configure the attributes of an ID Object\n* Country should be able to define the length of the UIN number",
            "title": "Configurability"
        },
        {
            "location": "/Architecture/#extensibility",
            "text": "MOSIP should be flexible to extend functionality on top of the basic platform. Some of the examples of extensibility are  A country should be able to introduce a new step in processing data  Integrate MOSIP with other ID systems and include it as part of the MOSIP data processing flow",
            "title": "Extensibility"
        },
        {
            "location": "/Architecture/#modularity",
            "text": "All components in MOSIP should be modular and their features exposed via interfaces such that the implementation behind the interface can be changed without affecting other modules. Some examples of modularity are  UIN generator algorithm provided by the platform can be replaced by a country with their own implementation  The default demographic deduplication algorithm provided by MOSIP can be changed to a different one without impacting the process flow",
            "title": "Modularity"
        },
        {
            "location": "/Architecture/#logical-architecture",
            "text": "Microservice based architecture for all platform services  Services must be stateless to scale out horizontally  Services must be idempotent  Services must have well defined interfaces     This will help achieve modularity, better maintainability and scalability.   Staged Event Driven Architecture (SEDA) for processing Registration data  Each processing step must be a stage  Stages must be connected with event bus    Process flow must be configured outside code   This will help achieve extensibility and scale out stages independently.   Thick client architecture for Registration client  Client must run on a desktop/laptop in offline mode also  Client must integrate with devices in a secure manner",
            "title": "Logical Architecture"
        },
        {
            "location": "/Architecture/#data-architecture",
            "text": "",
            "title": "Data Architecture"
        },
        {
            "location": "/Architecture/#open-source-and-vendor-neutral",
            "text": "To handle vendor neutrality and open source, the following consideration are followed while designing the data model and the database design.    No business logic is applied at database level: Database will be used only to store and retrieve data. There is no business logic applied at database level other than Primary / Unique key, Not null and foreign keys. Foreign keys are applied within the same database, if a table is referenced in another database then no FK is applied.     No specific database features to be used: Features that are common across databases which are compliant with open source standards are applied.     All DDL, DML and DQL statements will follow ANSI standards    Metadata approach to handle complex and flexible data structures    Only following datatypes are being used   Character varying  Timestamp  Date  Integer  Number  Bytea/blob  Boolean",
            "title": "Open Source and Vendor Neutral"
        },
        {
            "location": "/Architecture/#multi-language",
            "text": "MOSIP platform is being built for multiple countries, there is a need to support multiple languages. So as per the requirements, MOSIP will support 3 languages as configured by the country level administrator.\nMulti language support is needed for the following datasets.    Master Data  ID data of an individual  Transaction comments  Labels used in UI  Messages and notifications   From database side, the data will support  UTF-8 Unicode character set  to store data entered in multiple languages. \nThere will not be any in-built support to translate data at database level. Any translation or transliteration will be handled at API or UI layer.",
            "title": "Multi-Language"
        },
        {
            "location": "/Architecture/#high-performance",
            "text": "To support high performance, following database design features are to be considered   Database sharding is applied on uin dataset. By default, base sharding algorithm will be applied in MOSIP system. SI can define the sharding algorithm based on the deployment setup  All tables will have a primary key index on the primary key field. This will help in faster retrievals and joins  All foreign keys will have indexes defined so that it will help in faster joins  No referential integrity is applied on tables across databases  Partitioning: Partitioning design to be discussed as PostgreSQL has certain limitation / different way of implementation that requires specific database features to be applied. To be discussed further to finalize the implementation of this feature.",
            "title": "High Performance"
        },
        {
            "location": "/Technology-Stack/",
            "text": "Technology Stack\n\n\nThis page lists all the technologies used in building MOSIP\n\n\n\n\n\n\n\n\nCategory\n\n\nTool/Technology\n\n\nVersion\n\n\nLicense\n\n\nRemarks\n\n\n\n\n\n\n\n\n\n\nOperating System\n\n\nRed Hat\n\n\n7.5\n\n\n\n\n\n\n\n\n\n\nProgramming language\n\n\nJava\n\n\n1.8\n\n\nGNU V2\n\n\n\n\n\n\n\n\nApplication development framework\n\n\nSpring family\n\n\nvarious versions\n\n\nApache License 2.0\n\n\n\n\n\n\n\n\nMicroservices development framework\n\n\nSpring boot\n\n\n\n\nApache License 2.0\n\n\n\n\n\n\n\n\nThick client framework\n\n\nJavaFX\n\n\n9\n\n\nGPL V2 with linking exception\n\n\n\n\n\n\n\n\nWeb application framework\n\n\nAngular\n\n\n5\n\n\nMIT\n\n\n\n\n\n\n\n\nSEDA framework\n\n\nvert.x\n\n\n3.5.1\n\n\nApache License 2.0\n\n\n\n\n\n\n\n\nDatabase for server\n\n\nPostgreSQL\n\n\n10.5\n\n\nPostgreSQL license\n\n\nPermissive open source similar to BSD or MIT\n\n\n\n\n\n\nDatabase for client\n\n\nDerbyDB\n\n\n10.4\n\n\nApache License 2.0\n\n\n\n\n\n\n\n\nFile system storage\n\n\nCEPH\n\n\n13.2.0\n\n\nLGPL 2.1\n\n\n\n\n\n\n\n\nBuild tool\n\n\nApache Maven\n\n\n3.53\n\n\nApache License 2.0\n\n\n\n\n\n\n\n\nContainerization\n\n\nDocker\n\n\n18.03.x CE\n\n\nApache License 2.0\n\n\n\n\n\n\n\n\nReg-client-Automation\n\n\nTestFx\n\n\n4.0.15-alpha\n\n\nApache License 2.0\n\n\n\n\n\n\n\n\nReg-client-Automation\n\n\nAwaitility\n\n\n3.0.0\n\n\nApache License 2.0\n\n\n\n\n\n\n\n\nReg-client-Automation\n\n\nJunit-platform-launcher\n\n\n1.3.2\n\n\nEPL 2.0\n\n\n\n\n\n\n\n\nReg-client-Automation\n\n\nExtentreports\n\n\n2.41.2\n\n\nBSD 3-clause\n\n\n\n\n\n\n\n\nReg-client-Automation\n\n\nJunit-jupiter-engine\n\n\n5.0.0\n\n\nEPL 1.0\n\n\n\n\n\n\n\n\nReg-client-Automation\n\n\nApiguardian-api\n\n\n1.0.0\n\n\nApache 2.0\n\n\n\n\n\n\n\n\nRest API test automation\n\n\nRest Assured\n\n\n3.0.7\n\n\n\n\n\n\n\n\n\n\nFramework to run  test cases\n\n\nTestNG\n\n\n6.11\n\n\n\n\n\n\n\n\n\n\nTest Management\n\n\nZephyr\n\n\nCloud Jira & Zephyr\n\n\nCloud Jira & Zephyr",
            "title": "Technology Stack"
        },
        {
            "location": "/Technology-Stack/#technology-stack",
            "text": "This page lists all the technologies used in building MOSIP     Category  Tool/Technology  Version  License  Remarks      Operating System  Red Hat  7.5      Programming language  Java  1.8  GNU V2     Application development framework  Spring family  various versions  Apache License 2.0     Microservices development framework  Spring boot   Apache License 2.0     Thick client framework  JavaFX  9  GPL V2 with linking exception     Web application framework  Angular  5  MIT     SEDA framework  vert.x  3.5.1  Apache License 2.0     Database for server  PostgreSQL  10.5  PostgreSQL license  Permissive open source similar to BSD or MIT    Database for client  DerbyDB  10.4  Apache License 2.0     File system storage  CEPH  13.2.0  LGPL 2.1     Build tool  Apache Maven  3.53  Apache License 2.0     Containerization  Docker  18.03.x CE  Apache License 2.0     Reg-client-Automation  TestFx  4.0.15-alpha  Apache License 2.0     Reg-client-Automation  Awaitility  3.0.0  Apache License 2.0     Reg-client-Automation  Junit-platform-launcher  1.3.2  EPL 2.0     Reg-client-Automation  Extentreports  2.41.2  BSD 3-clause     Reg-client-Automation  Junit-jupiter-engine  5.0.0  EPL 1.0     Reg-client-Automation  Apiguardian-api  1.0.0  Apache 2.0     Rest API test automation  Rest Assured  3.0.7      Framework to run  test cases  TestNG  6.11      Test Management  Zephyr  Cloud Jira & Zephyr  Cloud Jira & Zephyr",
            "title": "Technology Stack"
        },
        {
            "location": "/MOSIP-Releases/",
            "text": "Latest Release\n\n\n\n\nVersion: 0.9.0 (Stable)\n\n\nName:  Proxy Biometrics\n\n\nDate:  01-Jul-2019\n\n\nRelease Notes\n\n\nSource code\n\n\nDocumentation",
            "title": "Releases"
        },
        {
            "location": "/MOSIP-Releases/#latest-release",
            "text": "Version: 0.9.0 (Stable)  Name:  Proxy Biometrics  Date:  01-Jul-2019  Release Notes  Source code  Documentation",
            "title": "Latest Release"
        },
        {
            "location": "/Roadmap/",
            "text": "July 2019 to June 2020\n\n\nSeed Contribution\n\n\n\n\nRepo Separation with CI/CD \n\n\nCommunity setup \n\n\nInterface mutation checks \n\n\nCommit auto tests \n\n\nCode promotion \n\n\nRelease process \n\n\n\n\n\n\nNative Launcher \n\n\nReal Biometric Release\n\n\nVanilla Platform \n\n\nComplete the stubbed config \n\n\nComplete modularity \n\n\n\n\n\n\nData Enrichment Rigidity\n\n\nSecurity Testing \n\n\nPerformance Testing \n\n\nMaintainability \n\n\nDatabase Sharding\n\n\nHigh Availability\n\n\nDisaster Recovery\n\n\nData Center Ops\n\n\n\n\n\n\nAdmin Application\n\n\nResident Services API\n\n\nPartner Management API\n\n\nID Auth Backlog\n\n\nReg Proc Backlog \n\n\nReg Client Backlog \n\n\nKernel Backlog Pre-Reg (UI cleanup, Captcha) \n\n\nReporting \n\n\n\n\nCommunity Contributions\n\n\n\n\nFunctional Backlog \n\n\nConfigurable UI \n\n\nIntegrations\n\n\neIDAS Connector \n\n\nCRVS Integration \n\n\n\n\n\n\nAnalytics \n\n\nFraud Management \n\n\neKYC Server \n\n\nA/B Testing\n\n\nCloud Setup\n\n\nDevice Management Server \n\n\nCompliance Certification Tools \n\n\nMobile App \n\n\nAndroid Reg Client \n\n\nIOS Reg Client \n\n\nOnline Registration \n\n\neKYC reference app",
            "title": "Roadmap"
        },
        {
            "location": "/Roadmap/#july-2019-to-june-2020",
            "text": "Seed Contribution   Repo Separation with CI/CD   Community setup   Interface mutation checks   Commit auto tests   Code promotion   Release process     Native Launcher   Real Biometric Release  Vanilla Platform   Complete the stubbed config   Complete modularity     Data Enrichment Rigidity  Security Testing   Performance Testing   Maintainability   Database Sharding  High Availability  Disaster Recovery  Data Center Ops    Admin Application  Resident Services API  Partner Management API  ID Auth Backlog  Reg Proc Backlog   Reg Client Backlog   Kernel Backlog Pre-Reg (UI cleanup, Captcha)   Reporting    Community Contributions   Functional Backlog   Configurable UI   Integrations  eIDAS Connector   CRVS Integration     Analytics   Fraud Management   eKYC Server   A/B Testing  Cloud Setup  Device Management Server   Compliance Certification Tools   Mobile App   Android Reg Client   IOS Reg Client   Online Registration   eKYC reference app",
            "title": "July 2019 to June 2020"
        },
        {
            "location": "/Getting-Started/",
            "text": "Getting Started\n\n\n\n\n1. Getting the Source Code\n\n\n2. Setup and Configure Jenkins\n\n\n3. Setup and Configure Jfrog Artifactory Version 6.5.2\n\n\n4. Setup and Configure SonarQube version 7.3\n\n\n5. Setup and Configure Docker Registry\n\n\n6. Installing External Dependencies\n\n\n7. Configuring MOSIP\n\n\n8. MOSIP Deployment\n\n\n\n\n1. Getting the Source Code \n[\u2191]\n\n\nKnowledge of Linux and Azure with Kubernetes are required to follow the instructions.\nMOSIP source code can be obtained via creating a fork of mosip-platform Github repository from the \nURL\n. To know more about how to fork code from Github follow this \nguide\n.\nOnce Forked, start the process of setting up your CI/CD tools to build and run MOSIP.\n\n\nNOTE\n MOSIP configuration has been seperated from the source code. For running the source code, you will be needing a fork of mosip-config repository from this \nURL\n. All the configuration files will be stored under config-templates folder under this repository.\n\n\n\n\n2. Setup and Configure Jenkins \n[\u2191]\n\n\nIn this step, we will setup jenkins and configure it. Configuration contains steps like creating credentials, creating pipelines using xml files present in MOSIP source code, connecting Jenkins to recently forked repository and creating webhooks. Lets look at these steps one by one -\n\n\nA. Installing Jenkins version 2.150.1\n\n\nJenkins installation is standard(see \nHow to install Jenkins\n), but to use MOSIP supported build pipelines you have to install Jenkins in an Redhat 7.6 environment. The prerequisite for installing Jenkins is you should have java already installed and path for JAVA_HOME is also set. Also the following plugins have to be installed\n list of plugins -\n\n \nGithub Plugin\n\n\n \nArtifactory Plugin\n\n\n \nCredentials Plugin\n\n\n \nDocker Pipeline Plugin\n\n\n \nEmail Extension Plugin\n\n\n \nPipeline Plugin\n\n\n \nPublish Over SSH Plugin\n\n\n \nSonarQube Scanner for Jenkins Plugin\n\n\n \nSSH Agent Plugin\n\n\n \nPipeline Utility Steps Plugin\n\n\n \nM2 Release Plugin\n\n\n \nSSH Credentials Plugin\n\n* \nOffice 365 Plugin\n\n\nOnce the plugin installation is complete, run this command in Jenkins Script Console -\n\n\nSystem.setProperty(\"hudson.model.DirectoryBrowserSupport.CSP\", \"\")\n\n\nThis above command modifies Content Security Policy in Jenkins to enable loading of style and javascript for HTML Reports.\n\n\nB. Setting Up Github for/in Jenkins\n\n\nSetting up Github for/in Jenkins involves putting the Jenkins Webhook url in Github Repo so that Github can inform Jenkins for push events(look at \nWebhooks\n and \nGithub hook\n). After hooks are in place, setup Github credentials inside Jenkins, so that on webhook event our pipeline can checkout the code from Github. To set up Github Credentials, follow these steps -\n\n\nI. Goto Jenkins\nII. Goto Credentials -> System\nIII. Goto Global credentials\nIV. Click on Add Credentials\nV. Now use following details\n\n    Kind=Username with password\n    Scope=Global (Jenkins, nodes, items, all child items, etc)\n    Username=<Your Github Username>\n    Password=<Your Github Password>\n    ID=Some Unique Identifier to refer to this credentials (to autogenerate this, leave this blank)\n    Description=<It is optional>\nVI. Now since our Jenkinsfile usage this github credentials, update the credentials id in the Jenkinsfile.\n\n\n\nC. Create Pipelines\n\n\nNext step after Jenkins installation is to configure/create Jenkins Jobs. These Jenkins Jobs are written as Jenkins Pipelines and respective Jenkinsfile in \nURL\n. MOSIP currently has 5 Jenkins jobs that take care of CI/CD process for Development Environment. They are -\n\n\n\n\n\n\nmaster-branch-build-all-modules\n\n\nJenkinsfile for master-branch-build-all-modules can be found in \nURL\n, named \nMasterJenkinsfile\n\nThis Job is used to build MOSIP as a single unit. This Job also acts like a nightly process to check the build status of MOSIP code in Master Branch. To create this Job you need to create a new Item in Jenkins as a Pipeline Project. Here is the configuration for Pipeline you might have to explicitly change to use MOSIP provided Jenkinsfile-\n\n\n\n\n\n\nAs it can be seen from the above image this pipeline uses Jenkinsfile present in master branch of mosip-platform repository. You need to provide the Github credentials that this pipeline will take to connect and download this Jenkinsfile at the time of the build. Let us now look into this Jenkinsfile.\n\n\n\n\n\n\nJenkinsfile for this pipeline is written in Groovy Language using the scripted style of writing code.\n\n\n\n\n\n\nThen we have module specific Jenkinsfile for individual Modules. These Modules are:\n\n\n\n\nKernel\n\n\nRegistration\n\n\nRegistration-Processor\n\n\nPre-Registration\n\n\nID-Repository\n\n\nID-Authentication\n\n\n\n\nEach Module's CI/CD Jenkins script can be found in \nURL\n. This Jenkins script will be  named Jenkinsfile and is responsible to build and deploy the entire Module to Dev environment\n\n\n\n\n\n\nFor promoting these modules to QA, there is a pipeline named \nPromoteToQAJenkinsFile\n which is located in root directory of mosip source code. This pipeline tags the entire code, runs build process, and once everything is successful, it deploys the entire code to QA environment.\n\n\n\n\n\n\nIn each Jenkinsfile you will see some variables starting with \nparams.\n These variables are passed as parameters into the Jenkins jobs. You have to setup these parameters variables in your jenkins to use these Jenkinsfiles. These Variables include:\n\n\nNOTE\n-> To set up parameters for a jenkins job, go inside the jenkins job-> Click Configure->Click on \"This project is parameterized\" and provide the parameter name and value.\n\n\n\n\nBRANCH_NAME\n\n\nREGISTRY_URL\n\n\nREGISTRY_NAME\n\n\nREGISTRY_CREDENTIALS\n\n\nGIT_URL\n\n\nGIT_CREDENTIALS\n\n\nBUIILD_OPTION\n\n\n\n\nNOTE\n We are building docker images in each of the JenkinsFile, so docker should be installed and accessible for Jenkins user. Please install Docker version 18.09.3 in your Jenkins instance.\n\n\n\n\n3. Setup and Configure Jfrog Artifactory Version 6.5.2 \n[\u2191]\n\n\nFor installing and setting up Jfrog, following steps \nhere\n need to be followed.\n\nOnce the setup is complete, please add the following remote repositories to your Jfrog configuration and point them to libs-release virtual repository:\n\n \nMaven Central\n\n\n \nJcentre\n\n* \nOpenimaj\n\n\nTo configure Maven to resolve artifacts through Artifactory you need to modify the settings.xml of Jenkins machine's m2_home to point to JFrog.\n\nTo generate these settings, go to  Artifact Repository Browser of the Artifacts module, select Set Me Up. In the Set Me Up dialog, set Maven in the Tool field and click \"Generate Maven Settings\". For more information on artifactory configuration refer \nhere\n\n\nNOTE\n JFrog Artifactory setup by MOSIP is open to public for read only access. So if any of the modules are dependent on previous modules, that you don't have built, you need to connect to our JFrog server to pull those dependencies. For doing that, in the settings.xml file that you generated above, replace url of ID with repository snapshot and release to our Jfrog URLs which will be : \n\n1. \n<url>http://<abcd>.mosip.io/artifactory/libs-snapshot</url>\n for libs-snapshot\n2. \n<url>http://<abcd>.mosip.io/artifactory/libs-release</url>\n for libs-release\n\n\nOnce you are done with pulling the dependencies you need, you can replace it back to your Jfrog URLs. \n\nAlso if you are planning to import all versions of the Mosip modules in Jfrog to your VM or Jfrog, make sure you have enough space in your Jfrog VM where you will be importing these dependencies.\n\n\n\n\n4. Setup and Configure SonarQube version 7.3 \n[\u2191]\n\n\nSonarQube server can be setup by following single instructions given \nhere\n.\n\nFor configuring SonarQube with Jenkins, steps given \nhere\n can be followed.\n\n\nSteps to install SonarQube on Ubuntu 16.04\n\n\n   Perform a system update\n\n\n * sudo apt-get update\n\n * sudo apt-get -y upgrade\n\n\n\n   Install JDK\n\n\n * sudo add-apt-repository ppa:webupd8team/java\n\n * sudo apt-get update\n\n * sudo apt install oracle-java8-installer\n\n\n\nWe can now check the version of Java by typing:\n\n\n * java -version\n\n\n\n   Install and configure PostgreSQL\n\n\n * sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >>\n   /etc/apt/sources.list.d/pgdg.list'\n\n * wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add -\n\n * sudo apt-get -y install postgresql postgresql-contrib\n\n\n\nStart PostgreSQL server and enable it to start automatically at boot time by running:\n\n\n * sudo systemctl start postgresql\n\n * sudo systemctl enable postgresql\n\n\n\nChange the password for the default PostgreSQL user.\n\n\n * sudo passwd postgres\n\n\n\nSwitch to the postgres user.\n\n\n * su - postgres\n\n\n\nCreate a new user by typing:\n\n\n * createuser sonar\n\n\n\nSwitch to the PostgreSQL shell.\n\n\n * psql\n\n\n\nSet a password for the newly created user for SonarQube database.\n\n\n * ALTER USER sonar WITH ENCRYPTED password 'StrongPassword';\n\n\n\nCreate a new database for PostgreSQL database by running:\n\n\n * CREATE DATABASE sonar OWNER sonar;\n\n\n\nExit from the psql shell:\n\n\n * \\q\n\n\n\nSwitch back to the sudo user by running the exit command.\n\n\n   Download and configure SonarQube\n\n\n * wget https://sonarsource.bintray.com/Distribution/sonarqube/sonarqube-6.4.zip\n\n\n\nInstall unzip by running:\n\n\n * apt-get -y install unzip\n\n\n\nUnzip the archive using the following command.\n\n\n * sudo unzip sonarqube-6.4.zip -d /opt\n\n\n\nRename the directory:\n\n\n * sudo mv /opt/sonarqube-6.4 /opt/sonarqube\n\n * sudo nano /opt/sonarqube/conf/sonar.properties\n\n\n\nFind the following lines.\n\n\n #sonar.jdbc.username=\n\n #sonar.jdbc.password=\n\n\n\nUncomment and provide the PostgreSQL username and password of the database that we have created earlier. It should look like:\n\n\n sonar.jdbc.username=sonar\n\n sonar.jdbc.password=StrongPassword\n\n\n\nNext, find:\n\n\nsonar.jdbc.url=jdbc:postgresql://localhost/sonar\n\n\nUncomment the line, save the file and exit from the editor.\n\n\n   Configure Systemd service\n\n\nSonarQube can be started directly using the startup script provided in the installer package. As a matter of convenience, you should setup a Systemd unit file for SonarQube.\n\n\n * nano /etc/systemd/system/sonar.service\n\n Populate the file with:\n\n [Unit]\n\n Description=SonarQube service\n\n After=syslog.target network.target\n\n [Service]\n\n Type=forking\n\n ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start\n\n ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop\n\n User=root\n\n Group=root\n\n Restart=always\n\n [Install]\n\n WantedBy=multi-user.target\n\n\n\nStart the application by running:\n\n\n * sudo systemctl start sonar\n\n\n\nEnable the SonarQube service to automatically start at boot time.\n\n\n * sudo systemctl enable sonar\n\n\n\nTo check if the service is running, run:\n\n\n * sudo systemctl status sonar\n\n\n\n\n\n5. Setup and Configure Docker Registry \n[\u2191]\n\n\nIn this step we will setup and configure a private docker registry, which will be basic authenticated, SSL secured. In our setup we are using azure blobs as storage for our docker images. More options for configuring registry can be found \nhere\n\nWe are deploying Docker registry as Containerized services. For setting up the registry, \nDocker\n and \nDocker Compose\n need to be installed. We have setted up the registry in a machine with Redhat 7.6 installed.\n\nOnce installation is done, the yaml files which we will be using to setup the registry can be found in this \nlink\n\nWe are using Registry image : registry:2.5.1, registry with any other version can be deployed from \nhere\n. \nFor routing purpose, we are using HAproxy image dockercloud/haproxy:1.6.2, other options such as ngnix etc. can also be used for the same purpose.\n\nWe have the following docker-compose files, under this \nlink\n\n1. \nregistry-docker-compose.yml:\n  For basic registry and haproxy setup.\n2. \nregistry-docker-compose-basic-authentication.yml:\n  For securing the docker registry through base authentication.\nFor basic authentication, you have to setup a htpasswd file and add a simple user to it. For generating this htpaswd file:\n\n  * Create Htpasswd_dir directory\n\n\nmkdir -p ~/htpasswd_dir\n\n  * Create htpasswd file with your username and password\n\n \ndocker run --rm --entrypoint htpasswd registry:2 -Bbn <username> \"<password>\" > ~/htpasswd_dir/htpasswd\n\n  * In the registry-docker-compose-basic-authentication.yml file, replace \n and \n with\n    specific values.\n\n\n\n\nregistry-docker-compose-azure-storage.yml:\n  This file is used for configuring azure blob storage. We are assuming that Azure blob has already been configured by you. Replace REGISTRY_STORAGE_AZURE_ACCOUNTNAME, REGISTRY_STORAGE_AZURE_ACCOUNTKEY, REGISTRY_STORAGE_AZURE_CONTAINER with appropriate values configured by you while setting up azure blob storage.\n\n\nregistry-docker-compose-tls-enabled.yml:\n  We are using \nLet's Encrypt\n, CA signed SSL certificates. Documentation of Let's Encrypt can be referred \nhere\n\n  Once Certificates have been generated, replace the \n property and \n property in registry-docker-compose-tls-enabled.yml with appropriate values.\nAfter completing all the above changes, use docker-compose tool to bring up the container using the following command:\n\n\ndocker-compose -f registry-docker-compose.yml -f registry-docker-compose-basic-authentication.yml -f registry-docker-compose-azure-storage.yml -f registry-docker-compose-tls-enabled.yml  up -d\n\nOnce the registry is up and running, variables \nregistryUrl\n, \nregistryName\n, \nregistryCredentials\n can be configured accordingly in Jenkinsfile.\n For configuring registry Credentials in Jenkins, Username/Password credentials need to be added in Jenkins Global Credentials and credential ID needs to be provided in \nregistryCredentials\n variable in all the Jenkinsfiles.\n\n\n\n\n\n\n6. Installing External Dependencies \n[\u2191]\n\n\n6.1 Install and use PostgreSql Version 10.2 on RHEL 7.5\n\n\nOften simply Postgres, is an object-relational database management system (ORDBMS) with an emphasis on extensibility and standards compliance. It can handle workloads ranging from small single-machine applications to large Internet-facing applications (or for data warehousing) with many concurrent users\nPostgresql Prerequisites\nOn a Linux or Mac system, you must have superuser privileges to perform a PostgreSQL installation. To perform an installation on a Windows system, you must have administrator privileges.\n\n\nSteps to install Postgresql in RHEL-7.5\n\n\nDownload and install PostgreSQL.\n\n\n$ sudo yum install https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-redhat10-10-2.noarch.rpm\n$ sudo  yum-config-manager --disable pgdg95\n\n\n\n\nchecking  the postgresql packages\n\n\n$ sudo yum update\n\n\n\n\n$ sudo yum list postgresql*\n\n\n##### Installation command\n\n\n\n\n$ sudo yum install postgresql10 postgresql10-server\n\n\n$sudo /usr/pgsql-10/bin/postgresql-10-setup initdb\n\n\n\n\n$sudo systemctl enable postgresql-10\n\n\n##### Postgresql service stop/start/restart command\n\n\n\n\n$ sudo systemctl start postgresql-10\n\n\n$ sudo systemctl status postgresql-10\n\n\n\n\n$ sudo systemctl stop postgresql-10\n\n\nTo changing default port 5432 to 9001 and connection + buffer size we need to edit the postgresql.conf file from below path\nPostgreSQL is running on default port 5432.\nyou decide to change the default port, please ensure that your new port number does not conflict with any services running on that port.\n\n##### Steps to change the default port :\n\n###### Open the file  and modify the below changes\n\n\n\n\n$ sudo vi /var/lib/pgsql/10/data/postgresql.conf\n\n\nlisten_addresses = '*'   (changed to * instead of local host )\nport = 9001       ( uncomment port=5432 and change the port number\n###### Open the port 9001 from the VM\n\n\n\n\n$  sudo firewall-cmd --zone=public --add-port=9001/tcp --permanent\n\n\n$  sudo firewall-cmd --reload\n\n\n\n\nTo increase the buffer size and number of postgreSql connection  same fine modify the below changes also\n\n\n$ sudo vi /var/lib/pgsql/10/data/postgresql.conf\n\n\n\n\nunix_socket_directories = '/var/run/postgresql, /tmp'\nmax_connections = 1000\n\nshared_buffers = 2GB  \n\n\n$ sudo systemctl start postgresql-10\n\n\n\n\nTo change the default password\n\n\nLogin to postgrsql\n\n\n$ sudo su postgres\n\n\n\n\nbash-4.2$    psql -p 9001\n\n\npostgres=# \\password postgres\n\n\n\n\nEnter new password:\n\n\nEnter it again:\n\n\n\n\npostgres=# \\q\n\n\nsudo systemctl restart postgresql-10\n\n\n\n\nIt will ask new password to login to postgresql\n\n\n# example  for sourcing the sql file form command line\n\n\n$ psql --username=postgres --host=<server ip> --port=9001 --dbname=postgres\n\n\n\n\nOpen the file \n\n$ sudo vim /var/lib/pgsql/10/data/pg_hba.conf\n\n\nDefault lines are present in pg_hab.conf file \n\n\nTYPE  DATABASE        USER            ADDRESS                 METHOD \n\n\nlocal   all             all                                     peer \n\nhost    all             all             127.0.0.1/32            ident \n\nhost    all             all             ::1/128                 ident \n\nlocal   replication     all                                     peer  \n\nhost    replication     all             127.0.0.1/32            ident \n\nhost    replication     all             ::1/128                 ident \n\n\nModify  with below changes in file  /var/lib/pgsql/10/data/pg_hba.conf\n\n\nlocal   all             all                                     md5 \n\nhost    all             all             127.0.0.1/32            ident \n\nhost    all             all             0.0.0.0/0               md5 \n\nhost    all             all             ::1/128                 ident \n\nlocal   replication     all                                     peer \n\nhost    replication     all             127.0.0.1/32            ident \n\nhost    replication     all             ::1/128                 ident \n\n\nsudo systemctl restart postgresql-10\n\n\n\n\nsudo systemctl status postgresql-10\n\n\n\n\nReference link:\nhttps://www.tecmint.com/install-postgresql-on-centos-rhel-fedora\n\n\n\n\n6.2 Install and use Nginx Version-1.15.8 on RHEL 7.5\n\n\nWe are using nginx for webserver andalso proxy server for MOSIP project\nCreate the file named /etc/yum.repos.d/nginx.repo using a text editor such as vim command\n\n\n$ sudo vi /etc/yum.repos.d/nginx.repo\n\n\n\n\nAppend following for RHEL 7.5\n\n\n[nginx]   \nname=nginx repo\nbaseurl=http://nginx.org/packages/mainline/rhel/7/$basearch/\ngpgcheck=0\nenabled=1\n\n\n\n\nAfter updating repo, please run following commands to install and enable nginx -\n\n\n$ sudo yum update\n\n\n\n\n$ sudo yum install nginx\n\n\n$ sudo systemctl enable nginx  \n\n\n\n\nTo start, stop, restart or get status of nginx use the following commands -\n\n\n$ sudo systemctl start nginx\n\n$ sudo systemctl stop nginx\n\n$ sudo systemctl restart nginx\n\n$ sudo systemctl status nginx\n\n\n\n\nTo edit files use a text editor such as vi\n\n\n$ sudo vi /etc/nginx/nginx.conf\n\n\n\n\nExample to  configure the nginx for dev environment -\n   ```\n    user  madmin;\n    worker_processes  2;\n    error_log  /var/log/nginx/error.log warn;\n    pid        /var/run/nginx.pid;\n    events {\n        worker_connections  1024;\n    }\n\n\nhttp {\n        include       /etc/nginx/mime.types;\n        default_type  application/octet-stream;\n        log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n        '$status $body_bytes_sent \"$http_referer\" '\n        '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n        access_log  /var/log/nginx/access.log  main;\n        client_max_body_size 10m;\n        sendfile        on;\n        tcp_nopush     on;\n        proxy_max_temp_file_size 0;\n        sendfile_max_chunk 10m;\n        keepalive_timeout  65;\n        gzip on;\n        gzip_disable \"msie6\";\n        gzip_vary on;\n        gzip_proxied any;\n        gzip_comp_level 6;\n        gzip_buffers 16 8k;\n        gzip_http_version 1.1;\n        gzip_min_length 256;\n        gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/javascript application/octet-stream application/xml+rss text/javascript application/vnd.ms-fontobject application/x-font-ttf font/opentype image/svg+xml image/x-icon image/png image/jpg;\n        #include /etc/nginx/conf.d/*.conf;\n\n\n\nHTTP configuration\n\n\n server {\n        listen  80 default_server;\n        listen [::]:80 default_server ipv6only=on;\n        server_name <your-domain-name>;\n\n        location / {\n                                    root /usr/share/nginx/html;\n                index  index.html index.htm;\n                proxy_http_version 1.1;\n                proxy_set_header Upgrade $http_upgrade;\n                proxy_set_header Connection \"upgrade\";\n                proxy_set_header Host $host;\n                proxy_connect_timeout                   3600s;\n                proxy_send_timeout                      3600s;\n                proxy_read_timeout                      3600s;\n        }\n        return 301 https://$host$request_uri;\n    }\n\n\n\nHTTPS configuration for your domain\n\n\n    server {\n\n            client_max_body_size 20M;\n                listen *:443 ssl http2;\n                listen [::]:443 ssl http2;\n                server_name dev.mosip.io;\n                ssl on;\n                ssl_certificate         <your-letsencrypt-fullchainpem-path>;\n                ssl_certificate_key   <your-letsencrypt-privatekey-pem-path>;\n        location /v1/keymanager/ {\n                    proxy_set_header Host $host;\n                    proxy_set_header X-Real-IP $remote_addr;\n                    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                    proxy_set_header X-Forwarded-Proto $scheme;\n                    #proxy_set_header Cookie $http_cookie;\n                    proxy_pass  http://<your-keymanager-vm-ip>:<port>/v1/keymanager/;\n            }\n\n             location /registrationprocessor/v1/packetreceiver/ {\n                    proxy_set_header Host $host;\n                    proxy_set_header X-Real-IP $remote_addr;\n                    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                    proxy_set_header X-Forwarded-Proto $scheme;\n                   proxy_pass  http://<your-dmz-vm-ip>:<port>/registrationprocessor/v1/packetreceiver/;\n            }\n\n             location /registrationprocessor/v1/registrationstatus/ {\n                    proxy_set_header Host $host;\n                    proxy_set_header X-Real-IP $remote_addr;\n                    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                    proxy_set_header X-Forwarded-Proto $scheme;\n                    proxy_pass  http://<your-dmz-vm-ip>:<port>/registrationprocessor/v1/registrationstatus/;\n\n            }\n\n            location / {\n                    proxy_set_header Host $host;\n                    proxy_set_header X-Real-IP $remote_addr;\n                    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                    #proxy_set_header X-Forwarded-Host $proxy_add_x_forwarded_for;\n                    proxy_set_header X-Forwarded-Proto $scheme;\n                    proxy_connect_timeout                   3600s;\n                    proxy_send_timeout                      3600s;\n                    proxy_read_timeout                      3600s;\n                    proxy_pass https://<your-dev-k8-cluster-endpoint>; //kubernetes end point\n                    #proxy_intercept_errors on;\n                    #error_page 301 302 307 = @handle_redirects;\n\n            }\n }\n}\n\n\n\n```\n\n\nUse below command to open the port 80/443 from RHEL 7.5 VM\n\n\n$ sudo firewall-cmd --zone=public --add-port=80/tcp --permanent\n$ sudo firewall-cmd --zone=public --add-port=443/tcp --permanent\n$ sudo firewall-cmd --reload\n\n\n\n\nGenerate SSL/TLS for HTTPS -\n\n\nRHEL 7  version, these are the following commands you have to run to generate certificates for nginx server.\n\n\n\n\n\n\nwget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n              It will install EPEL script for RHEL 7 OS. It is same as PPA for Ubuntu, it will install some extra packages for enterprise linux edition.\nYou can read more about it https://fedoraproject.org/wiki/EPEL.\n\n\n\n\n\n\nThis command will run the EPEL install scripts and enable the EPEL packages for RHEL7\n      \nsudo yum install epel-release-latest-7.noarch.rpm\n\n\n\n\n\n\nThis will list all the EPEL packages available for used.\n      \nyum --disablerepo=\"*\" --enablerepo=\"epel\"  list available\n\n\n\n\n\n\nCheck for python2-certbot-nginx package in EPEL Packages.\n    \nyum --disablerepo=\"*\" --enablerepo=\"epel\" search python2-certbot-nginx\n\n\n\n\n\n\nThis will install python certbot for nginx into VM.\n    \nsudo yum install python2-certbot-nginx\n\n\n\n\n\n\nThis will generate the certificate for VM.\n    \nsudo certbot --nginx certonly\n\n\n\n\n\n\nTroubleshooting:\n If you facing getting this issue in nginx \n (13: Permission denied) while connecting to upstream:[nginx] \n\nPlease  run below command -\n\n\n$sudo setsebool -P httpd_can_network_connect 1\n\n\n\n\nor refer link -\nhttps://stackoverflow.com/questions/23948527/13-permission-denied-while-connecting-to-upstreamnginx\n\n\nNote: Certficates will be generated at, /etc/letsencrypt/live/\n/ directory. cert.pem is the certificate and privkey.pem is private key. We are using Let's Encrypt, CA signed SSL certificates. Documentation of Let's Encrypt can be referred \nhere\n\n\n6.3 Clam AntiVirus Version 0.101.0\n\n\nClamAV is a free, cross-platform and open-source antivirus software toolkit able to detect many types of malicious software, including viruses.\n\n\nSteps to install ClamAV in RHEL-7.5\n\n\nTo install clamAV first we need to install EPEL Repository:\n\n\n$ sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n\n\n\n\nAfter that we need to install ClamAV and its related tools.\n\n\n$ yum -y install clamav-server clamav-data clamav-update clamav-filesystem clamav clamav-scanner-systemd clamav-devel clamav-lib clamav-server-systemd\n\n\n\n\nAfter completion of above steps, we need to configure installed ClamAV. This can be done via editing \n/etc/clamd.d/scan.conf\n. In this file we have to remove \nExample\n lines. So that ClamAV can use this file's configurations. We can easily do it via running following command -\n\n\n$ sed -i '/^Example/d' /etc/clamd.d/scan.conf\n\n\n\n\nAnother thing we need to do in this file is to define our TCP server type. Open this file using -\n\n\n$ vim /etc/clamd.d/scan.conf\n\n\n\n\nhere this we need to uncomment line with \n#LocalSocket /var/run/clamd.scan/clamd.sock\n. Just remove \n#\n symbol from the beginning of the line.\n\n\nNow we need to configure FreshClam so that it can update ClamAV db automatically. For doing that follow below steps -\n\n\nFirst create a backup of original FreshClam Configuration file -\n\n\n$ cp /etc/freshclam.conf /etc/freshclam.conf.bak\n\n\n\n\nIn this \nfreshclam.conf\n file, Here also we need to remove \nExample\n line from the file. Run following command to delete all \nExample\n lines-\n\n\n$ sed -i '/^Example/d' /etc/freshclam.conf\n\n\n\n\nTest freshclam via running-\n\n\n$ freshclam\n\n\n\n\nAfter running above command you should see an output similar to this -\n\n\nClamAV update process started at Thu May 23 07:25:44 2019\n.\n.\n.\n.\nmain.cvd is up to date (version: 58, sigs: 4566249, f-level: 60, builder: sigmgr)\nDownloading daily-25584.cdiff [100%]\ndaily.cld updated (version: 25584, sigs: 1779512, f-level: 63, builder: raynman)\nbytecode.cld is up to date (version: 331, sigs: 94, f-level: 63, builder: anvilleg)\nDatabase updated (6345855 signatures) from database.clamav.net (IP: 104.16.218.84)\n\n\n\n\nWe will create a service of freshclam so that freshclam will run in the daemon mode and periodically check for updates throughout the day. To do that we will create a service file for freshclam -\n\n\n$ vim /usr/lib/systemd/system/clam-freshclam.service\n\n\n\n\nAnd add below content -\n\n\n[Unit]\nDescription = freshclam scanner\nAfter = network.target\n\n[Service]\nType = forking\nExecStart = /usr/bin/freshclam -d -c 4\nRestart = on-failure\nPrivateTmp = true\nRestartSec = 20sec\n\n[Install]\nWantedBy=multi-user.target\n\n\n\n\nNow save and quit. Also reload the systemd daemon to refresh the changes -\n\n\n$ systemctl daemon-reload\n\n\n\n\nNext start and enable the freshclam service -\n\n\n$ systemctl start clam-freshclam.service\n\n$ systemctl enable clam-freshclam.service\n\n\n\n\nNow freshclam setup is complete and our ClamAV db is upto date. We can continue setting up ClamAV. Now we will copy ClamAV service file to system service folder.\n\n\n$ mv /usr/lib/systemd/system/clamd@.service /usr/lib/systemd/system/clamd.service\n\n\n\n\nSince we have changed the name, we need to change it at the file that uses this service as well -\n\n\n$ vim /usr/lib/systemd/system/clamd@scan.service\n\n\n\n\nRemove @ symbol from \n.include /lib/systemd/system/clamd@.service\n line and save the file.\n\n\nWe will edit Clamd service file now -\n\n\n$ vim /usr/lib/systemd/system/clamd.service\n\n\n\n\nAdd following lines at the end of clamd.service file.\n\n\n[Install]\nWantedBy=multi-user.target\n\n\n\n\nAnd also remove \n%i\n symbol from various locations (ex: Description and ExecStart options). Note that at the end of the editing the service file should look something like this -\n\n\n[Unit]\nDescription = clamd scanner daemon\nDocumentation=man:clamd(8) man:clamd.conf(5) https://www.clamav.net/documents/\n# Check for database existence\n# ConditionPathExistsGlob=@DBDIR@/main.{c[vl]d,inc}\n# ConditionPathExistsGlob=@DBDIR@/daily.{c[vl]d,inc}\nAfter = syslog.target nss-lookup.target network.target\n\n[Service]\nType = forking\nExecStart = /usr/sbin/clamd -c /etc/clamd.d/scan.conf\nRestart = on-failure\n\n[Install]\nWantedBy=multi-user.target\n\n\n\n\nNow finally start the ClamAV service.\n\n\n$ systemctl start clamd.service\n\n\n\n\nIf it works fine, then enable this service and test the status of ClamAV service -\n\n\n$ systemctl enable clamd.service\n\n$ systemctl status clamd.service\n\n\n\n\nNow in MOSIP we require ClamAV to be available on Port 3310. To expose ClamAV service on Port 3310, edit \nscan.conf\n\n\n$ vi /etc/clamd.d/scan.conf\n\n\n\n\nand Uncomment \n#TCPSocket 3310\n by removing \n#\n. After that restart the clamd@scan service -\n\n\n$ systemctl restart clamd@scan.service\n\n\n\n\nSince we are exposing ClamAV on 3310 port, we need to allow incoming traffic through this port. In RHEL 7 run below command to add firewall rule -\n\n\n$ sudo firewall-cmd --zone=public --add-port=3310/tcp --permanent\n$ sudo firewall-cmd --reload\n\n\n\n\nReference link: \nlink\n\n\n6.4 Steps to Install and configuration CEPH\n\n\nNOTE: Required only if CEPH is used for packet storage.\nCeph is an open source software that provides massively scalable and distributed data store. It provides highly scalable object, block and file based storage under a unified system.\n\n\n1. On Red Hat Enterprise Linux 7, register the target machine with subscription-manager, verify your subscriptions, and enable the \u201cExtras\u201d repository for package dependencies. For example:\n\n\n$ sudo subscription-manager repos --enable=rhel-7-server-extras-rpms\n\n\n\n2. Install and enable the Extra Packages for Enterprise Linux (EPEL) repository:\n\n\n$ sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n\n\n\n3. Add the Ceph repository to your yum configuration file at /etc/yum.repos.d/ceph.repo with the following command.  Replace {ceph-stable-release} with a stable Ceph release (e.g., luminous.) For example:\n\n\ncat << EOM > /etc/yum.repos.d/ceph.repo\n[ceph-noarch]\nname=Ceph noarch packages\nbaseurl=https://download.ceph.com/rpm-{ceph-stable-release}/el7/noarch\nenabled=1  \ngpgcheck=1\ntype=rpm-md\ngpgkey=https://download.ceph.com/keys/release.asc\nEOM\n\n\n\n4. Update your repository and install ceph-deploy:\n\n\n$ sudo yum update\n$ sudo yum install ceph-deploy\n\n\n\nCEPH NODE SETUP\n\n\nThe admin node must have password-less SSH access to Ceph nodes. When ceph-deploy logs in to a Ceph node as a user, that particular user must have passwordless sudo privileges.\n\n\n\nINSTALL NTP\n\n\nWe recommend installing NTP on Ceph nodes (especially on Ceph Monitor nodes) to prevent issues arising from clock drift. See  [Clock](//docs.ceph.com/docs/mimic/rados/configuration/mon-config-ref/#clock) for details.\n$ sudo yum install ntp ntpdate ntp-doc\n    Ensure that you enable the NTP service. Ensure that each Ceph Node uses the same NTP time server.\n\n\n\nINSTALL SSH SERVER\n\n\nsudo yum install openssh-server\n    Ensure the SSH server is running on ALL Ceph Nodes.\n\n\n\n1) Make a directory on admin node in order to keep all the keys and configuration files that ceph-deploy generates.\n    a. mkdir cluster-config\n    b. cd cluster-config\n2) Now we create a cluster\n\n\nceph-deploy new {initial-monitor-node(s)}\n\nceph-deploy new ceph-demo-1 ceph-demo-2\n\n\n\nThis step marks the nodes as initial monitors.\n\n\n3) Thereafter, we Install ceph packages on required nodes\n\n\nceph-deploy install {ceph-node} [\u2026]\n\nceph-deploy install ceph-demo-1 ceph-demo-2\n\n\n\nThis step will install the latest stable version of ceph, i.e. mimic (13.2.1) on the given nodes.\n\n\n4) Now, we deploy the initial monitor nodes and gather keys\n\n\nceph-deploy mon create-initial\n\n\n\n5) Now we go ahead and copy our config file and admin key to the admin node as well as ceph-nodes in order to use ceph cli without passing these each time we execute a command.\n\n\nceph-deploy admin {ceph-node}[\u2026]\n\nceph-deploy admin ceph-demo-1 ceph-demo-2\n\n\n\n6) Now, we deploy a manager daemon\n\n\nceph-deploy mgr create {ceph-node}[\u2026]\n\nceph-deploy mgr create ceph-demo-1 ceph-demo-2\n\n\n\n7) We create 2 OSDs, assuming each osd has a unused disk called dev/sdb\n\n\nceph-deploy osd create\u200a\u2014\u200adata {device} {ceph-node}\n\nceph-deploy osd create \u2014 data /dev/sdb ceph-demo-1\nceph-deploy osd create \u2014 data /dev/sdb ceph-demo-2\n\n\n\nAfter successfully executing these steps, our ceph cluster is up and running. The status and health of the cluster can be checked in by executing\n\n\n$ sudo ceph health\n$ sudo ceph -s\n\n\n\nWe should get a status saying HEALTH_OK, and a detailed status resembling :\n\n\ncluster:\n\nid: 651e9802-b3f0\u20134b1d-a4d6-c57a46635bc9\n\nhealth: HEALTH_OK\n\nservices:\n\nmon: 2 daemons, quorum ceph-demo-1,ceph-demo-2\n\nmgr: ceph-demo-1(active), standbys: ceph-demo-2\n\nosd: 2 osds: 2 up, 2 in\n\ndata:\n\npools: 0 pools, 0 pgs\n\nobjects: 0 objects, 0 B\n\nusage: 2.0 GiB used, 18 GiB / 20 GiB avail\n\npgs:\n\n\n\nExpanding the Existing cluster\n\n\nNow, to demonstrate the ease of expanding a ceph cluster at runtime, we will be adding one node in our running cluster. We will mark that node as osd, manager and monitor to increase the availability of our existing cluster.\nFirst of all, we need to make a change to our existing ceph.conf which is present inside the cluster-config directory. We add the following line into it\n\n\npublic network = {ip-address}/{bits}\n\n\npublic network = 10.142.0.0/24\n\n\nFor this, we need to follow these sample steps:\n\n\n1) We install ceph packages on 3rd node\n\n\nceph-deploy install {ceph-node}[\u2026]\n\nceph-deploy install ceph-demo-3\n\n\n\n2) We need to push the admin keys and conf to 3rd node. We do it using\n\n\nceph-deploy admin {ceph-node}[\u2026]\n\nceph-deploy admin ceph-demo-3\n\n\n\n3) Now we will add the 3rd node as our monitor\n\n\nceph-deploy mon add {ceph-nodes}\n\nceph-deploy mon add ceph-demo-3\n\n\n\n4) Now, we go ahead and mark 3rd node as our manager\n\n\nceph-deploy mgr create {ceph-node}[\u2026]\n\nceph-deploy mgr create ceph-demo-3\n\n\n\n5) We add 3rd node as OSD by following same steps as done while creating the cluster.\n\n\nceph-deploy osd create\u200a\u2014\u200adata {path} {ceph-node}\n\n\n\nCEPH Dashboard\n\n\nNow, going ahead, we can enable the CEPH dashboard in order to view all the cluster status via a UI console.\n\n\nCeph in its mimic release has provided the users with a new and redesigned dashboard plugin, with the features like restricted control with username/password protection and SSL/TLS support.\n\n\nTo enable the dashboard, we need to follow these steps:\n\n\n1) ceph mgr module enable dashboard\n\n2) ceph dashboard create-self-signed-cert\n\n\n\nNote: Self signed certificate is only for quick start purpose.\n\n\n3) ceph mgr module disable dashboard\n\n4) ceph mgr module enable dashboard\n\n\n\nNow, we will be able to see the CEPH dashboard on the port 8443, which is by default but on requirement can be configured using:\n\n\nceph config set mgr mgr/dashboard/server_addr $IP\n\nceph config set mgr mgr/dashboard/server_port $PORT\n\n\n\nTo access, and to utilize full functionality of the dashboard, we need to create the login credentials.\n\n\nceph dashboard set-login-credentials <username> <password>\n\n\n\nAfter these steps, our ceph infrastructure is ready with all the configurations to do some actual input output operations.\n\n\nReference link:\n\n\nhttp://docs.ceph.com/docs/mimic/start/quick-start-preflight/\n\n\n6.5 Steps to Install and configuration LDAP\n\n\nApacheDs Server installation and config\n\n\nApache Directory Studio user guide\n\n\n6.6 Steps to Install and configuration HDFS\n\n\nNOTE: Required only if HDFS is used for packet storage.\n\n\nRefer - Steps-to-Install-and-configuration-HDFS\n\n\n6.7 Steps to Deploy Kernel Key Manager Service\n\n\nKernel Keymanager Service is setup outside of Kubernetes cluster on a standalone VM. The steps to setup kernel-keymanager-service are given \nhere\n\n\nTo deploy keymanager service, follow below steps -\n1.  Prerequiste:\n\n       *  A machine with RHEL 7.6 installed.\n       * Docker installed and Docker service enabled.\n\n\nSteps to install Docker ce.\n\n\n $ sudo yum install http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.107-3.el7.noarch.rpm\n\n $ sudo yum -y install lvm2 device-mapper device-mapper-persistent-data device-mapper-event device-mapper-libs    device-mapper-event-libs\n\n$ sudo wget https://download.docker.com/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\n\n$ sudo yum -y install docker-ce\n\n$ sudo systemctl start docker\n\n$ sudo systemctl status docker\n\n\n\n\n\nOpen port 8088 on the VM:\n\n\n\n\nsudo firewall-cmd --zone=public --add-port=8088/tcp --permanent\n\nsudo firewall-cmd --reload\n\n\n\n\nNote:\n if firewall is not installed in VM, install with \u201csudo yum install firewall\u201d\n\n\nAnd also open the port.\n\n\n\n\nensure that config server is already deployed.\n\n\n\n\nProcess to deploy Services in VM through JenkinsFile:\n\n\n\n\nRefer the github url for Jenkinsfile : in root directory of kernel module\n\n\n\n\nThe last stage in the Jenkinsfile viz 'Key-Manager Deployment' in which we are sshing into this newly created VM through Jenkins to deploy this service, basically, running the docker image of key manager.\n\n\nFor ssh, place the public key of jenkins inside this newly created VM's authorized_keys under .ssh directory. Generate Docker Registry Credential in jenkins by using docker hub username and password. This will generate the credentialsId.\n\n\n\n\n\n\nReplace the value for registryCredentials(credentialsId of docker hub) with yours\n\n\n\n\n\n\nReplace the value for  key_manager_vm_ip with IP of your newly created VM.\n\n\n\n\n\n\nOnce done the following command will be used to deploy keymanager to the machine: \n\n\nsudo docker run -tid --ulimit memlock=-1 -p 8088:8088 -v softhsm:/softhsm -e spring_config_url_env=\"${config_url}\" -e spring_config_label_env=\"${branch}\" -e active_profile_env=\"${profile_env}\" --name keymanager \"${registryAddress}\"/kernel-keymanager-service\n\n\n\n\n**NOTE- Replace the values for spring_config_url_env, spring_config_label_env,\n      active_profile_env and registryAddress in the above command accordingly\n\n\n6.8 SMS Gateway configuration\n\n\nRefer kernel-smsnotification-servive Readme \nhere\n\n\n6.9 Installation of ActiveMQ\n\n\nActiveMQ is the message broker used for MOSIP Registartion processor module.\n\n\nInstallation steps\n\n\n\n\n<version>\n : please check http://www.apache.org/dist/activemq/ to find out the latest version. Tested ActiveMQ version - 5.4.1.\n\n\nPrerequiste:\n\n        A machine with RHEL 7.6 installed, Docker installed and Docker service enabled.\n\n\nDownload activemq using command - \n\n\nwget https://archive.apache.org/dist/activemq/5.14.3/apache-activemq-5.14.3-bin.tar.gz\n\n\nExtract the archive \n\n\ntar -zxvf apache-activemq-<version>-bin.tar.gz\n\n\nChange the permission for startup script\n\n\nchmod 755 apache-activemq-<version>\n\n\nStart activemq service\n\n\ncd apache-activemq-<version> && sudo  ./bin/activemq start\n\n\nCheck for the installed and started activemq on port 61616. \n\n\nnetstat -tulpn\n\n\nOpen ports 8161 and 61616  on the VM:\n\n\n\n\nsudo firewall-cmd --zone=public --add-port=8161/tcp --permanent\nsudo firewall-cmd --zone=public --add-port=61616/tcp --permanent\nsudo firewall-cmd --reload\n\n\n\n\nNote:\n After Installation of activemq, same needs to be mentioned in RegistrationProcessorAbis_{active_profile}.json\nFor e.g : Suppose activemq is configured as tcp://xxx.xxx.xxx.xx:61616, then we for dev need to mention this in RegistrationProcessorAbis_dev.json as\n\n\n{\n    \"abis\": [{\n            \"name\": \"ABIS1\",\n            \"host\": \"\",\n            \"port\": \"\",\n            \"brokerUrl\": \"tcp://xxx.xxx.xxx.xx:61616\",\n            \"inboundQueueName\": \"abis1-inbound-address_dev\",\n            \"outboundQueueName\": \"abis1-outbound-address_dev\",\n            \"pingInboundQueueName\": \"\",\n            \"pingOutboundQueueName\": \"\",\n            \"userName\": \"admin\",\n            \"password\": \"admin\",\n                \"typeOfQueue\": \"ACTIVEMQ\"\n        }\n    ]\n\n}\n\n\n\n\nActiveMQ is also being used in registration-processor-printing-stage and the details need to be mentioned in registration-processor-{active_profile}.properties in the configuration repository.\nE.g : For dev profile, the property in registration-processor-dev.properties, the Property corresponding to printing-stage related to activemq would be\n\n\nQueue username\nregistration.processor.queue.username={username}\n#Queue Password\nregistration.processor.queue.password={password}\n#Queue Url\nregistration.processor.queue.url={queue_url}\n#Type of the Queue\nregistration.processor.queue.typeOfQueue=ACTIVEMQ\n#Print Service address\nregistration.processor.queue.address={queue_address}\n#Post Service address\nregistration.processor.queue.printpostaladdress={postal_queue_address}\n\n\n\n\n7. Configuring MOSIP \n[\u2191]\n\n\nMOSIP database object deployment / configuration\n\n\nDatabase deployment consists of the following 4 categories of objects to be deployed on postgresql database.\n\n\n\n\n\n\nUser / Roles:\n In MOSIP, the following user / roles are defined to perform various activities\n\n\n\n\n\n\nsysadmin:\n sysadmin user/role is a super administrator role, who will have all the privileges to performa any task within the database.\n\n\n\n\n\n\ndbadmin:\n dbadmin user / role is created to handle all the database administration activities db monitoring, performance tuning, backups, restore, replication setup, etc.\n\n\n\n\n\n\nappadmin:\n appadmin user / role is used to perform all the DDL (Data Definition Language) tasks. All the db objects that are created in these databases will be owned by appadmin user.\n\n\n\n\n\n\nApplication User:\n Each application will have a user / role created to perform DML (Data Manipulation Language) tasks like CRUD operations (select, insert, update, delete). The user prereguser, is created to connect from the application to perform all the DML activities. Similarly, we will have masteruser, prereguser, reguser, idauser, idrepouser, kerneluser, audituser, regprcuser to perform DML tasks for master, pre-registration, registration, ida, ID repository, kernel, audit and registration processor modules respectively.\n\n\n\n\n\n\nNote:\n From the above set of roles only application user / role is specific to a application / module. The other user / roles are common which needs to be  created per postresql db instance / server.\n\n\n\n\n\n\nDatabase and Schema:\n Each application / module of MOSIP platform will have a database and schema defined. All the objects (tables) related to an application / module would be created under the respective database / schema. In MOSIP the following database and scehmas are defined\n\n\n\n\n\n\n\n\n\n\n\n\napplication / module name\n\n\nDatabase tool\n\n\ndatabase Name\n\n\nschema name\n\n\n\n\n\n\n\n\n\n\nMaster / Administration module\n\n\npostgresql\n\n\nmosip_master\n\n\nmaster\n\n\n\n\n\n\nKernel\n\n\npostgresql\n\n\nmosip_kernel\n\n\nkernel\n\n\n\n\n\n\nPre-registration\n\n\npostgresql\n\n\nmosip_prereg\n\n\nprereg\n\n\n\n\n\n\nRegistration\n\n\nApache Derby\n\n\nmosip_reg\n\n\nreg\n\n\n\n\n\n\nRegistration Processor\n\n\npostgresql\n\n\nmosip_regprc\n\n\nregprc\n\n\n\n\n\n\nID Authentication\n\n\npostgresql\n\n\nmosip_ida\n\n\nida\n\n\n\n\n\n\nID Repository\n\n\npostgresql\n\n\nmosip_idrepo\n\n\nidrepo\n\n\n\n\n\n\nAudit\n\n\npostgresql\n\n\nmosip_audit\n\n\naudit\n\n\n\n\n\n\nIAM\n\n\npostgresql\n\n\nmosip_iam\n\n\niam\n\n\n\n\n\n\nidmap\n\n\npostgresql\n\n\nmosip_idmap\n\n\nidmap\n\n\n\n\n\n\n\n\nNote:\n These databases can be deployed on single or separate database servers / instances.\n\n\n\n\n\n\nDB Objects (Tables):\n All the tables of each application / module will be created in their respective database and schema. appadmin user / role will own these objects and the respective application user / role will have access to perform DML operations on these objects.\n\n\n\n\n\n\nSeed Data:\n MOSIP platform is designed to provide most of its features to be configured in the system. These configuration are deployed with default setup on config server and few in database. Few of these configuration can be modified / updated by the MOSIP administrator. These configuration include, system configurations, master datasetup, etc. The steps to add new center, machine / device is detailed in \nGuidelines for Adding Centers, Machine, Users and Devices\n\n\n\n\n\n\nThe system configuration and master data is available under the respective application / database related folder. for example, the master data configuration is available in csv file format under \nfolder\n.\n\n\nThe scripts to create the above objects are available under \ndb_scripts\n. To deploy the database objects of each application / module \nexcept registration client\n, please refer to \nREADME.MD\n file. These scripts will contain the deployment of all the DB object categories.\n\n\nNote: Please skip Registration client related deployment scripts (Apache derby DB specific) as this will be executed as part of registration client software installation.\n\n\nSetup and configure MOSIP\n\n\nWe are using kubernetes configuration server in MOSIP for storing and serving distributed configurations across all the applications and environments.\n\n\nWe are storing all applications' configuration in config-templates folder inside our Github Repository \nhere\n.\n\n\nFor getting more details about how to use configuration server with our applications, following developer document can be referred:\n\nMOSIP CONFIGURATION SERVER\n\n\nFor Deployment of configurations server, go to \nfirstly-deploy-kernel-configuration-server\n in this document.\n\n\nApplication specific configuration for all applications and services are placed in MOSIP config server.\n\n\nA. Global:\n\n\nlink\n\n\nB. Kernel:\n\n\nlink\n\n\nC. Pre-Registration:\n\n\nlink\n\n\nD. Registartion-Processor:\n\n\nlink\n\n\nE. IDA:\n\n\nlink\n\n\nF. ID-REPO:\n\n\nlink\n\n\nH. Registration:\n\n\nlink\n\n\nProperties Sections that need to be changed in above module specific files once the external dependencies are installed as per your setup\n\n\nGlobal\n\n\n\n\n\n\n--Common properties------------\n\n\n\n\n\n\nmosip.base.url\n\n\n\n\n\n\n\n\n--Virus Scanner-----------------\n\n\n\n\n\n\nmosip.kernel.virus-scanner.host\nmosip.kernel.virus-scanner.port\n\n\n\n\n\n\n\n\n--FS Adapter-HDFS -------------\n\n\n\n\n\n\nmosip.kernel.fsadapter.hdfs.name-node-url\n# Enable if hadoop security authorization is 'true', default is false\nmosip.kernel.fsadapter.hdfs.authentication-enabled\n# If HDFS is security is configured with Kerberos, Key Distribution Center domain\nmosip.kernel.fsadapter.hdfs.kdc-domain\n#keytab file path, must be set if authentication-enable is true\n#read keytab file both classpath and physical path ,append appropriate prefix\n#for classpath prefix classpath:mosip.keytab\n#for physical path prefix file:/home/keys/mosip.keytab\nmosip.kernel.fsadapter.hdfs.keytab-file=classpath:mosip.keytab\n\n\n\n\nKernel\n\n\n\n\n\n\n--kernel common properties-----------------------\n\n\n\n\n\n\nmosip.kernel.database.hostname\nmosip.kernel.database.port\n\n\n\n\n\n\n\n\n--sms notification service-----------------------\n\n\n\n\n\n\nmosip.kernel.sms.authkey\n\n\n\n\n\n\n\n\n--Email Notification service---------------------\n\n\n\n\n\n\nspring.mail.host\nspring.mail.username\nspring.mail.password\nspring.mail.port=587\n\n\n\n\n\n\n\n\n--Ldap------------\n\n\n\n\n\n\nldap_1_DS.datastore.ipaddress\nldap_1_DS.datastore.port\n\n\n\n\n\n\n\n\n--DataBase Properties----------------------------\n\n\n\n\n\n\n**_database_password\n**_database_username\n\n\n\n\n\n\n8. MOSIP Deployment \n[\u2191]\n\n\nCurrently for the Development Process MOSIP Platform is deployed as/in the Kubernetes Cluster. We are using Azure Kubernetes Service for provisioning of Cluster. As of now Kubernetes Deployment is deviced in two parts -\n\n\nA. One time setup of MOSIP in Kubernetes Cluster\n\n\nB. Continuous deployment\n\n\nA. One time setup of MOSIP in Kubernetes Cluster\n\n\nOne time setup on Kubernetes involves following Steps \n\nI. Setting Up local system to communicate with Kubernetes cluster this can be done via \nkubectl\n.\n\nII. Setting Up the Basic environment for MOSIP to run in Kubernetes Cluster, In this step we will work on this \nlink\n . following are the files -\n\n\n\n\nWe will now go through each of the file and see what changes we need to perform. we will be using \nkubectl\n to do the deployments from local system.\n\n\n\n\nDeployIngressController.yaml - We need not to change anything here. we can directly run this file. To run this use this command\n\nkubectl apply -f DeployIngressController.yaml\n\n\n\n\nDeployIngress.yaml -\nThis file contains information about routing to different Kubernetes services, So whenever any traffic comes to our Load Balancer IP it will look for this file to route the request. For eg. Let's say if \nsome.example.com\n is mapped to our kubernetes loadbalancer then if a request is for \nsome.example.com/pre-registration-ui\n then this request will be redirect to \npre-registration-ui\n on port \n80\n service. Routes referrring to \nping-server\n and \nsample-nginx\n can be removed as these are for testing purpose.To run this use this command\n\nkubectl apply -f DeployIngress.yaml\n\n\n\n\n\n\nDeployServiceIngressService.yaml - We need not to change anything here. we can directly run this file. To run this use this command\nkubectl apply -f DeployServiceIngressService.yaml\n\n\n\n\n\n\nDeployDefaultBackend.yaml - We need not to change anything here. we can directly run this file. To run this use this command\n\nkubectl apply -f DeployDefaultBackend.yaml\n\n\n\n\n\n\ndocker-registry-secret.yml -\nThis file helps Kubernetes to get the Docker Images from Private Docker Registry. This file is a downloaded YAML of secrets that exists in the Kubernetes. You can either create secret or use this file to deploy secret in Kubernetes. For creating secret for the first time, run below command -\n\n\n\n\n\n\nkubectl create secret docker-registry <registry-credential-name> --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>\n\n\nOnce secret is created on the Kubernetes Cluster, as a backup strategy we can download the created secret using this command\n\n\nkubectl get secret <registry-credential-name> -o yaml --export\n\n\nOnce the above deployment is done, we will start deploying MOSIP services. For doing this, we need to look for these directories -\n\n\n\n\nFirstly Deploy Kernel Configuration server\n\n\nThe script is inside ( https://github.com/mosip/mosip-infra/blob/master/deployment/cloud/kubernetes/configuration-server/config-server-deployment-and-service.yml ) \n\nFollow below steps:\n1. Create a ssh key and configure it with your git repository. If you have already configured the ssh key for your repository, you can use that one or else follow \nthis\n \n\n2. Create a secret for Config server to connect to GIT repo. This secret contains your \nid_rsa key (private key), id_rsa_pub key (public key) and known_hosts\n which you generated above. We need this secret because config server connects to your Source code management repository, to get configuration for all the services(If you are using ssh URL for cloning the repo). For generating the required secret give the following command: ( Firstly try to connect to GIT repository from your system using ssh url and the key you created above, so that GIT service provider such as GitHub or GitLab comes in your known hosts file): \n\n\n`kubectl create secret generic config-server-secret --from-file=id_rsa=/path/to/.ssh/id_rsa --from-file=id_rsa.pub=/path/to/.ssh/id_rsa.pub --from-file=known_hosts=/path/to/.ssh/known_hosts` <br/>\n\n**For Encryption Decryption of properties with configuration server** <br/>\n<br/>\nCreate keystore with following command:\n\n`keytool -genkeypair -alias <your-alias> -keyalg RSA -keystore server.keystore -keypass < key-secret > -storepass < store-password > --dname \"CN=<your-CN>,OU=<OU>,O=<O>,L=<L>,S=<S>,C=<C>\"`\n\n\n\n\n\nThe JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format, migrate it using following command:\n\n\nkeytool -importkeystore -srckeystore server.keystore -destkeystore server.keystore -deststoretype pkcs12\n \n\nFor more information look \nhere\n \n\n\n\n\n\n\nCreate file with following content to create keystore secret for encryption decryption of keys using information from keystore created above: \n\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: config-server-keystore-values-secret\ntype: Opaque\ndata:\n  alias: < base-64-encoded-alias-for keystore >\n  password: <  base-64-store-password >\n  secret: < base-64-encoded-key-secret >\n\n5. Save the above file with any name and apply it using: \n\n\nkubectl apply -f < file-name >\n\n\n\n\n\n\n\n\nCreate server.keystore as secret to volume mount it inside container: \n\n\nkubectl create secret generic config-server-keystore --from-file=server.keystore=< location-of-your-server.keystore-file-generated-above >\n\n\n\n\n\n\n\n\nChange \ngit_url_env\n environment variable in kernel-config-server-deployment-and-service.yml to your git ssh url of configuration repository\n\n\n\n\n\nChange \ngit_config_folder_env\n environment variable in kernel-config-server-deployment-and-service.yml  to your configuration folder in git repository.\n\n\n\n\n\nChange \nspec->template->spec->containers->image\n from \ndocker-registry.mosip.io:5000/kernel-config-server\n to \n< Your Docker Registry >/kernel-config-server\n \n\n\n\n\nChange \nspec->template->spec->imagePullSecrets->name\n from \npvt-reg-cred\n to \n< Your docker registry credentials secret >\n\n\n\n\n\n\nOnce above configuration is done, execute \nkubectl apply -f kernel-config-server-deployment-and-service.yml\n\n\n\n\n\n\n\n\nMore information can be found \nhere\n\n\n\n\nDeploy other components: \n\n\nInside each of the directory there is a file for each service of MOSIP that is exposed as Web API. We need to deploy these files to get these running. But before doing that we need to change Private Docker Registry Address and Docker Registry Secret, so that on deployment time Kubernetes can fetch docker images from correct source using correct credentials.\nFor doing this, follow below steps (for eg. we will use https://github.com/mosip/mosip-infra/blob/master/deployment/cloud/kubernetes/kernel-deployment/kernel-auditmanager-service-deployment-and-service.yml, but you have to repeat the process for all such files) - \n\nI. Open a deployment file. \n\nII. Change \nspec->template->spec->containers->image\n from \ndocker-registry.mosip.io:5000/kernel-auditmanager-service\n to \n<Your Docker Registry>/kernel-auditmanager-service\n \n\nIII. Change \nspec->template->spec->imagePullSecrets->name\n from \npvt-reg-cred\n to \n<Your docker registry credentials secret>\n \n\nIV. Change \nactive_profile_env\n to whichever profile you want to activate and \nspring_config_label_env\n to the branch from which you want to pick up the configuration\n\nV. Save the file and Run \nkubectl apply -f kernel-auditmanager-service-deployment-and-service.yml\n \n\n\nAfter above process is completed, you can run \nkubectl get services\n command to see the status of all the MOSIP Services.\n\n\nFor Pre-Registration-UI\n\nPre-registration-ui uses a file config.json to configure URLs of backend, which have to be provided as config map in pre-registration-ui-deployment-and-service.yml. For creating the configmap follow below steps:\n1. Edit the file scripts -> https://github.com/mosip/mosip-infra/blob/master/deployment/cloud/kubernetes/pre-registration-deployment/pre-registration-ui-configuration.yaml\n2. Update \nhttps://dev.mosip.io/\n value with url of proxy server which points to pre-registration services. (Note: While editing, be careful with escape sequence characters)\n3. Execute command \nKubectl apply -f pre-registration-ui-configuration.yaml\n\n\n\n\n8.1  Registration-Processor DMZ services deployment\n\n\nRegistration Processor DMZ Services are setup externally(deployed in a separate VM).\n\n\nFirstly, update below files present in config folder in configuration repository, and replace the line\n\n \n<to uri=\"https://<dns name>/registrationprocessor/v1/uploader/securezone\" />\n\n with the URL of packet uploader stage.\n\n 1.  \nregistration-processor-camel-routes-new-dmz-<env-name>.xml\n\n 2.  \nregistration-processor-camel-routes-update-dmz-<env-name>.xml\n\n 3.  \nregistration-processor-camel-routes-lost-dmz-<env-name>.xml\n\n 4.  \nregistration-processor-camel-routes-activate-dmz-<env-name>.xml\n\n 5.  \nregistration-processor-camel-routes-deactivate-dmz-<env-name>.xml\n\n 6.  \nregistration-processor-camel-routes-res_update-dmz-<env-name>.xml\n\n\nWe are deploying DMZ services into another VM having docker installed. The steps to setup DMZ environment and services deployment:\n1. Need to set Up VM with RHEL 7.6\n2. Installing the Docker ce:\n\n\n      $ sudo yum install http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.107-\n        3.el7.noarch.rpm\n\n     $ sudo yum -y install lvm2 device-mapper device-mapper-persistent-data device-mapper-event device-mapper-libs\n       device-mapper-event-libs\n\n     $ sudo wget https://download.docker.com/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\n\n     $ sudo yum -y install docker-ce\n\n     $ sudo systemctl start docker\n\n     $ sudo systemctl status docker\n\n\n\n\n\nNeed to copy the Jenkins server public key(id_rsa.pub) inside this newly created VM's authorized_keys(because through jenkins job, we will ssh into new VM and deploy)\n\n\n\n\nAfter installing Docker Start the Docker Service\n\n\ncommand to start the Docker service\n\n\n\n\nsystemctl start docker\n\n\n\n\ncommand to check Docker is running:\n\n\n\n\n\n\nsystemctl status docker\n\n\n\n\n\n\nOpen the port 8081, 8083 from the VM:\n\nMosip uses port 8081 for registration-processor-packet-receiver-stage and 8083 for registration-processor-registration-status-service. The port ids need to be updated in ngnix configuration.\n\n\n\n\n\n\nsudo firewall-cmd --zone=public --add-port=8081/tcp --permanent\n\n\nsudo firewall-cmd --reload\n\n\nsudo firewall-cmd --zone=public --add-port=8083/tcp --permanent\n\n\nsudo firewall-cmd --reload\n\n\nNote:\n if firewall is not installed in VM, install with \u201csudo yum install firewall\u201d\n\n\nAnd also open the port from AZURE OR AWS or any cloud where the VM is launched.\n\n\nProcess to deploy Services in VM through JenkinsFile:\n\n\n\n\nThe last stage in the Jenkinsfile viz DMZ_Deployment in which we are sshing into this newly created VM through Jenkins to deploy these services, basically, running the docker images of registration processor.\nChanges to be made in this stage->\n\n\n\n\na. Replace the value for registryCredentials(credentialsId of docker hub) with yours.\n\n\nb. Replace the value for the variable -> dmz_reg_proc_dev_ip with the IP of your newly created VM.\n\n\nRefer the github url for Jenkinsfile : \nhere\n\n\n\n\nAlso, instead of following as described in 4th point to use Jenkinsfile, we can do it manually. Steps are ->\n\n\n\n\na. Login into the DMZ VM.\n\n\nb. Perform docker hub login\n\n\nc. Execute the following commands\n\n\ndocker run --restart always -it -d -p 8083:8083 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" \"${registryAddress}\"/registration-processor-registration-status-service\n\n\n\n\ndocker run --restart always -it -d --network host --privileged=true -v /home/ftp1/LANDING_ZONE:/home/ftp1/LANDING_ZONE -v /home/ftp1/ARCHIVE_PACKET_LOCATION:/home/ftp1/ARCHIVE_PACKET_LOCATION -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" \"${registryAddress}\"/registration-processor-packet-receiver-stage\n\n\n\n\ndocker run --restart always -it -d --network host --privileged=true -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e zone_env=dmz  \"${registryAddress}\"/registration-processor-common-camel-bridge\n\n\n\n\nNote\n - Please change the environmental variables(active_profile_env, spring_config_label_env, spring_config_url_env ,registryAddress) in the above commands accordingly whether you are executing manually in your new VM or through Jenkinsfile.\n\n\n\n\nPacket uploader stage in secure zone will fetch file from dmz to upload it into Distributed File System,to connect to\n   dmz vm either we can login using username and password or using ppk file.\n   If password value is available in config property name\n   registration.processor.dmz.server.password then uploader will connect using username and password.\n   otherwise it will login using ppk file available in config with property name registration.processor.vm.ppk.\n   PPK generation command ssh-keygen -t rsa -b 4096 -f mykey.\n\n\n\n\n8.2 Kernel Salt Generator\n\n\nKernel Salt Generator Job is a one-time job which is run to populate salts to be used to hash and encrypt data. This generic job takes schema and table name as input, and generates and populates salts in the given schema and table.\n\n\nSalt Generator Deployment steps\n\n\na. Login into the VM.\n     Open the port 8092 from the VM:\n\n\nsudo firewall-cmd --zone=public --add-port=8092/tcp --permanent\n\n\nsudo firewall-cmd --reload\n\n\nAnd also open the port from AZURE OR AWS or any cloud where the VM is launched.\n\n\nb. Perform docker hub login\n\n\nc. Execute the following commands sequentially one after the other. Wait for the completion of previous command before\n     executing next commands.\n\n\n  1.  docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\"  \n     -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.identity.db.shard.url -e schema_name=idrepo -e table_name=uin_hash_salt\n     \"${registryAddress}\"/id-repository-salt-generator\n\n  2.  docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\"  \n     -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.identity.db.shard.url -e schema_name=idrepo -e table_name=uin_encrypt_salt\n     \"${registryAddress}\"/id-repository-salt-generator\n\n  3.  docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\"  \n     -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.vid.db.url -e schema_name=idmap -e table_name=uin_hash_salt\n     \"${registryAddress}\"/id-repository-salt-generator\n\n  4.  docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\"       \n     -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.vid.db.url -e schema_name=idmap -e table_name=uin_encrypt_salt\n     \"${registryAddress}\"/id-repository-salt-generator\n\n  5.  docker run -it -d -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\"\n      -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-authentication -e schema_name=ida -e table_name=uin_hash_salt\n      \"${registryAddress}\"/authentication-salt-generator\n\n  6. docker run -it -d -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\"\n     -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-authentication -e schema_name=ida -e table_name=uin_encrypt_salt\n     \"${registryAddress}\"/authentication-salt-generator\n\n\n\nNote\n - Please change the value for variables active_profile_env, spring_config_label_env, spring_config_url_env and registryAddress in the above four commands accordingly\n\n\n8.3 First User Registration and Onboarding\n\n\nRefer to wiki for detailed procedure on First User Registration and Onboarding",
            "title": "Getting Started"
        },
        {
            "location": "/Getting-Started/#getting-started",
            "text": "1. Getting the Source Code  2. Setup and Configure Jenkins  3. Setup and Configure Jfrog Artifactory Version 6.5.2  4. Setup and Configure SonarQube version 7.3  5. Setup and Configure Docker Registry  6. Installing External Dependencies  7. Configuring MOSIP  8. MOSIP Deployment",
            "title": "Getting Started"
        },
        {
            "location": "/Getting-Started/#1-getting-the-source-code",
            "text": "Knowledge of Linux and Azure with Kubernetes are required to follow the instructions.\nMOSIP source code can be obtained via creating a fork of mosip-platform Github repository from the  URL . To know more about how to fork code from Github follow this  guide .\nOnce Forked, start the process of setting up your CI/CD tools to build and run MOSIP.  NOTE  MOSIP configuration has been seperated from the source code. For running the source code, you will be needing a fork of mosip-config repository from this  URL . All the configuration files will be stored under config-templates folder under this repository.",
            "title": "1. Getting the Source Code [\u2191]"
        },
        {
            "location": "/Getting-Started/#2-setup-and-configure-jenkins",
            "text": "In this step, we will setup jenkins and configure it. Configuration contains steps like creating credentials, creating pipelines using xml files present in MOSIP source code, connecting Jenkins to recently forked repository and creating webhooks. Lets look at these steps one by one -",
            "title": "2. Setup and Configure Jenkins [\u2191]"
        },
        {
            "location": "/Getting-Started/#a-installing-jenkins-version-21501",
            "text": "Jenkins installation is standard(see  How to install Jenkins ), but to use MOSIP supported build pipelines you have to install Jenkins in an Redhat 7.6 environment. The prerequisite for installing Jenkins is you should have java already installed and path for JAVA_HOME is also set. Also the following plugins have to be installed\n list of plugins -   Github Plugin    Artifactory Plugin    Credentials Plugin    Docker Pipeline Plugin    Email Extension Plugin    Pipeline Plugin    Publish Over SSH Plugin    SonarQube Scanner for Jenkins Plugin    SSH Agent Plugin    Pipeline Utility Steps Plugin    M2 Release Plugin    SSH Credentials Plugin \n*  Office 365 Plugin  Once the plugin installation is complete, run this command in Jenkins Script Console -  System.setProperty(\"hudson.model.DirectoryBrowserSupport.CSP\", \"\")  This above command modifies Content Security Policy in Jenkins to enable loading of style and javascript for HTML Reports.",
            "title": "A. Installing Jenkins version 2.150.1"
        },
        {
            "location": "/Getting-Started/#b-setting-up-github-forin-jenkins",
            "text": "Setting up Github for/in Jenkins involves putting the Jenkins Webhook url in Github Repo so that Github can inform Jenkins for push events(look at  Webhooks  and  Github hook ). After hooks are in place, setup Github credentials inside Jenkins, so that on webhook event our pipeline can checkout the code from Github. To set up Github Credentials, follow these steps -  I. Goto Jenkins\nII. Goto Credentials -> System\nIII. Goto Global credentials\nIV. Click on Add Credentials\nV. Now use following details\n\n    Kind=Username with password\n    Scope=Global (Jenkins, nodes, items, all child items, etc)\n    Username=<Your Github Username>\n    Password=<Your Github Password>\n    ID=Some Unique Identifier to refer to this credentials (to autogenerate this, leave this blank)\n    Description=<It is optional>\nVI. Now since our Jenkinsfile usage this github credentials, update the credentials id in the Jenkinsfile.",
            "title": "B. Setting Up Github for/in Jenkins"
        },
        {
            "location": "/Getting-Started/#c-create-pipelines",
            "text": "Next step after Jenkins installation is to configure/create Jenkins Jobs. These Jenkins Jobs are written as Jenkins Pipelines and respective Jenkinsfile in  URL . MOSIP currently has 5 Jenkins jobs that take care of CI/CD process for Development Environment. They are -    master-branch-build-all-modules  Jenkinsfile for master-branch-build-all-modules can be found in  URL , named  MasterJenkinsfile \nThis Job is used to build MOSIP as a single unit. This Job also acts like a nightly process to check the build status of MOSIP code in Master Branch. To create this Job you need to create a new Item in Jenkins as a Pipeline Project. Here is the configuration for Pipeline you might have to explicitly change to use MOSIP provided Jenkinsfile-    As it can be seen from the above image this pipeline uses Jenkinsfile present in master branch of mosip-platform repository. You need to provide the Github credentials that this pipeline will take to connect and download this Jenkinsfile at the time of the build. Let us now look into this Jenkinsfile.    Jenkinsfile for this pipeline is written in Groovy Language using the scripted style of writing code.    Then we have module specific Jenkinsfile for individual Modules. These Modules are:   Kernel  Registration  Registration-Processor  Pre-Registration  ID-Repository  ID-Authentication   Each Module's CI/CD Jenkins script can be found in  URL . This Jenkins script will be  named Jenkinsfile and is responsible to build and deploy the entire Module to Dev environment    For promoting these modules to QA, there is a pipeline named  PromoteToQAJenkinsFile  which is located in root directory of mosip source code. This pipeline tags the entire code, runs build process, and once everything is successful, it deploys the entire code to QA environment.    In each Jenkinsfile you will see some variables starting with  params.  These variables are passed as parameters into the Jenkins jobs. You have to setup these parameters variables in your jenkins to use these Jenkinsfiles. These Variables include:  NOTE -> To set up parameters for a jenkins job, go inside the jenkins job-> Click Configure->Click on \"This project is parameterized\" and provide the parameter name and value.   BRANCH_NAME  REGISTRY_URL  REGISTRY_NAME  REGISTRY_CREDENTIALS  GIT_URL  GIT_CREDENTIALS  BUIILD_OPTION   NOTE  We are building docker images in each of the JenkinsFile, so docker should be installed and accessible for Jenkins user. Please install Docker version 18.09.3 in your Jenkins instance.",
            "title": "C. Create Pipelines"
        },
        {
            "location": "/Getting-Started/#3-setup-and-configure-jfrog-artifactory-version-652",
            "text": "For installing and setting up Jfrog, following steps  here  need to be followed. \nOnce the setup is complete, please add the following remote repositories to your Jfrog configuration and point them to libs-release virtual repository:   Maven Central    Jcentre \n*  Openimaj  To configure Maven to resolve artifacts through Artifactory you need to modify the settings.xml of Jenkins machine's m2_home to point to JFrog. \nTo generate these settings, go to  Artifact Repository Browser of the Artifacts module, select Set Me Up. In the Set Me Up dialog, set Maven in the Tool field and click \"Generate Maven Settings\". For more information on artifactory configuration refer  here  NOTE  JFrog Artifactory setup by MOSIP is open to public for read only access. So if any of the modules are dependent on previous modules, that you don't have built, you need to connect to our JFrog server to pull those dependencies. For doing that, in the settings.xml file that you generated above, replace url of ID with repository snapshot and release to our Jfrog URLs which will be :  \n1.  <url>http://<abcd>.mosip.io/artifactory/libs-snapshot</url>  for libs-snapshot\n2.  <url>http://<abcd>.mosip.io/artifactory/libs-release</url>  for libs-release  Once you are done with pulling the dependencies you need, you can replace it back to your Jfrog URLs.  \nAlso if you are planning to import all versions of the Mosip modules in Jfrog to your VM or Jfrog, make sure you have enough space in your Jfrog VM where you will be importing these dependencies.",
            "title": "3. Setup and Configure Jfrog Artifactory Version 6.5.2 [\u2191]"
        },
        {
            "location": "/Getting-Started/#4-setup-and-configure-sonarqube-version-73",
            "text": "SonarQube server can be setup by following single instructions given  here . \nFor configuring SonarQube with Jenkins, steps given  here  can be followed.  Steps to install SonarQube on Ubuntu 16.04     Perform a system update   * sudo apt-get update\n\n * sudo apt-get -y upgrade     Install JDK   * sudo add-apt-repository ppa:webupd8team/java\n\n * sudo apt-get update\n\n * sudo apt install oracle-java8-installer  We can now check the version of Java by typing:   * java -version     Install and configure PostgreSQL   * sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >>\n   /etc/apt/sources.list.d/pgdg.list'\n\n * wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add -\n\n * sudo apt-get -y install postgresql postgresql-contrib  Start PostgreSQL server and enable it to start automatically at boot time by running:   * sudo systemctl start postgresql\n\n * sudo systemctl enable postgresql  Change the password for the default PostgreSQL user.   * sudo passwd postgres  Switch to the postgres user.   * su - postgres  Create a new user by typing:   * createuser sonar  Switch to the PostgreSQL shell.   * psql  Set a password for the newly created user for SonarQube database.   * ALTER USER sonar WITH ENCRYPTED password 'StrongPassword';  Create a new database for PostgreSQL database by running:   * CREATE DATABASE sonar OWNER sonar;  Exit from the psql shell:   * \\q  Switch back to the sudo user by running the exit command.     Download and configure SonarQube   * wget https://sonarsource.bintray.com/Distribution/sonarqube/sonarqube-6.4.zip  Install unzip by running:   * apt-get -y install unzip  Unzip the archive using the following command.   * sudo unzip sonarqube-6.4.zip -d /opt  Rename the directory:   * sudo mv /opt/sonarqube-6.4 /opt/sonarqube\n\n * sudo nano /opt/sonarqube/conf/sonar.properties  Find the following lines.   #sonar.jdbc.username=\n\n #sonar.jdbc.password=  Uncomment and provide the PostgreSQL username and password of the database that we have created earlier. It should look like:   sonar.jdbc.username=sonar\n\n sonar.jdbc.password=StrongPassword  Next, find:",
            "title": "4. Setup and Configure SonarQube version 7.3 [\u2191]"
        },
        {
            "location": "/Getting-Started/#sonarjdbcurljdbcpostgresqllocalhostsonar",
            "text": "Uncomment the line, save the file and exit from the editor.     Configure Systemd service  SonarQube can be started directly using the startup script provided in the installer package. As a matter of convenience, you should setup a Systemd unit file for SonarQube.   * nano /etc/systemd/system/sonar.service\n\n Populate the file with:\n\n [Unit]\n\n Description=SonarQube service\n\n After=syslog.target network.target\n\n [Service]\n\n Type=forking\n\n ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start\n\n ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop\n\n User=root\n\n Group=root\n\n Restart=always\n\n [Install]\n\n WantedBy=multi-user.target  Start the application by running:   * sudo systemctl start sonar  Enable the SonarQube service to automatically start at boot time.   * sudo systemctl enable sonar  To check if the service is running, run:   * sudo systemctl status sonar",
            "title": "sonar.jdbc.url=jdbc:postgresql://localhost/sonar"
        },
        {
            "location": "/Getting-Started/#5-setup-and-configure-docker-registry",
            "text": "In this step we will setup and configure a private docker registry, which will be basic authenticated, SSL secured. In our setup we are using azure blobs as storage for our docker images. More options for configuring registry can be found  here \nWe are deploying Docker registry as Containerized services. For setting up the registry,  Docker  and  Docker Compose  need to be installed. We have setted up the registry in a machine with Redhat 7.6 installed. \nOnce installation is done, the yaml files which we will be using to setup the registry can be found in this  link \nWe are using Registry image : registry:2.5.1, registry with any other version can be deployed from  here .  For routing purpose, we are using HAproxy image dockercloud/haproxy:1.6.2, other options such as ngnix etc. can also be used for the same purpose. \nWe have the following docker-compose files, under this  link \n1.  registry-docker-compose.yml:   For basic registry and haproxy setup.\n2.  registry-docker-compose-basic-authentication.yml:   For securing the docker registry through base authentication.\nFor basic authentication, you have to setup a htpasswd file and add a simple user to it. For generating this htpaswd file: \n  * Create Htpasswd_dir directory  mkdir -p ~/htpasswd_dir \n  * Create htpasswd file with your username and password \n  docker run --rm --entrypoint htpasswd registry:2 -Bbn <username> \"<password>\" > ~/htpasswd_dir/htpasswd \n  * In the registry-docker-compose-basic-authentication.yml file, replace   and   with\n    specific values.   registry-docker-compose-azure-storage.yml:   This file is used for configuring azure blob storage. We are assuming that Azure blob has already been configured by you. Replace REGISTRY_STORAGE_AZURE_ACCOUNTNAME, REGISTRY_STORAGE_AZURE_ACCOUNTKEY, REGISTRY_STORAGE_AZURE_CONTAINER with appropriate values configured by you while setting up azure blob storage.  registry-docker-compose-tls-enabled.yml:   We are using  Let's Encrypt , CA signed SSL certificates. Documentation of Let's Encrypt can be referred  here \n  Once Certificates have been generated, replace the   property and   property in registry-docker-compose-tls-enabled.yml with appropriate values.\nAfter completing all the above changes, use docker-compose tool to bring up the container using the following command:  docker-compose -f registry-docker-compose.yml -f registry-docker-compose-basic-authentication.yml -f registry-docker-compose-azure-storage.yml -f registry-docker-compose-tls-enabled.yml  up -d \nOnce the registry is up and running, variables  registryUrl ,  registryName ,  registryCredentials  can be configured accordingly in Jenkinsfile.  For configuring registry Credentials in Jenkins, Username/Password credentials need to be added in Jenkins Global Credentials and credential ID needs to be provided in  registryCredentials  variable in all the Jenkinsfiles.",
            "title": "5. Setup and Configure Docker Registry [\u2191]"
        },
        {
            "location": "/Getting-Started/#6-installing-external-dependencies",
            "text": "",
            "title": "6. Installing External Dependencies [\u2191]"
        },
        {
            "location": "/Getting-Started/#61-install-and-use-postgresql-version-102-on-rhel-75",
            "text": "Often simply Postgres, is an object-relational database management system (ORDBMS) with an emphasis on extensibility and standards compliance. It can handle workloads ranging from small single-machine applications to large Internet-facing applications (or for data warehousing) with many concurrent users\nPostgresql Prerequisites\nOn a Linux or Mac system, you must have superuser privileges to perform a PostgreSQL installation. To perform an installation on a Windows system, you must have administrator privileges.",
            "title": "6.1 Install and use PostgreSql Version 10.2 on RHEL 7.5"
        },
        {
            "location": "/Getting-Started/#steps-to-install-postgresql-in-rhel-75",
            "text": "",
            "title": "Steps to install Postgresql in RHEL-7.5"
        },
        {
            "location": "/Getting-Started/#download-and-install-postgresql",
            "text": "$ sudo yum install https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-redhat10-10-2.noarch.rpm\n$ sudo  yum-config-manager --disable pgdg95",
            "title": "Download and install PostgreSQL."
        },
        {
            "location": "/Getting-Started/#checking-the-postgresql-packages",
            "text": "$ sudo yum update  $ sudo yum list postgresql*  ##### Installation command  $ sudo yum install postgresql10 postgresql10-server  $sudo /usr/pgsql-10/bin/postgresql-10-setup initdb  $sudo systemctl enable postgresql-10  ##### Postgresql service stop/start/restart command  $ sudo systemctl start postgresql-10  $ sudo systemctl status postgresql-10  $ sudo systemctl stop postgresql-10  To changing default port 5432 to 9001 and connection + buffer size we need to edit the postgresql.conf file from below path\nPostgreSQL is running on default port 5432.\nyou decide to change the default port, please ensure that your new port number does not conflict with any services running on that port.\n\n##### Steps to change the default port :\n\n###### Open the file  and modify the below changes  $ sudo vi /var/lib/pgsql/10/data/postgresql.conf  listen_addresses = '*'   (changed to * instead of local host )\nport = 9001       ( uncomment port=5432 and change the port number\n###### Open the port 9001 from the VM  $  sudo firewall-cmd --zone=public --add-port=9001/tcp --permanent  $  sudo firewall-cmd --reload",
            "title": "checking  the postgresql packages"
        },
        {
            "location": "/Getting-Started/#to-increase-the-buffer-size-and-number-of-postgresql-connection-same-fine-modify-the-below-changes-also",
            "text": "$ sudo vi /var/lib/pgsql/10/data/postgresql.conf  unix_socket_directories = '/var/run/postgresql, /tmp'\nmax_connections = 1000 \nshared_buffers = 2GB    $ sudo systemctl start postgresql-10",
            "title": "To increase the buffer size and number of postgreSql connection  same fine modify the below changes also"
        },
        {
            "location": "/Getting-Started/#to-change-the-default-password",
            "text": "Login to postgrsql  $ sudo su postgres  bash-4.2$    psql -p 9001  postgres=# \\password postgres  Enter new password:  Enter it again:  postgres=# \\q  sudo systemctl restart postgresql-10  It will ask new password to login to postgresql",
            "title": "To change the default password"
        },
        {
            "location": "/Getting-Started/#example-for-sourcing-the-sql-file-form-command-line",
            "text": "$ psql --username=postgres --host=<server ip> --port=9001 --dbname=postgres  Open the file  \n$ sudo vim /var/lib/pgsql/10/data/pg_hba.conf",
            "title": "# example  for sourcing the sql file form command line"
        },
        {
            "location": "/Getting-Started/#default-lines-are-present-in-pg_habconf-file",
            "text": "TYPE  DATABASE        USER            ADDRESS                 METHOD   local   all             all                                     peer  \nhost    all             all             127.0.0.1/32            ident  \nhost    all             all             ::1/128                 ident  \nlocal   replication     all                                     peer   \nhost    replication     all             127.0.0.1/32            ident  \nhost    replication     all             ::1/128                 ident",
            "title": "Default lines are present in pg_hab.conf file "
        },
        {
            "location": "/Getting-Started/#modify-with-below-changes-in-file-varlibpgsql10datapg_hbaconf",
            "text": "local   all             all                                     md5  \nhost    all             all             127.0.0.1/32            ident  \nhost    all             all             0.0.0.0/0               md5  \nhost    all             all             ::1/128                 ident  \nlocal   replication     all                                     peer  \nhost    replication     all             127.0.0.1/32            ident  \nhost    replication     all             ::1/128                 ident   sudo systemctl restart postgresql-10  sudo systemctl status postgresql-10  Reference link:\nhttps://www.tecmint.com/install-postgresql-on-centos-rhel-fedora",
            "title": "Modify  with below changes in file  /var/lib/pgsql/10/data/pg_hba.conf"
        },
        {
            "location": "/Getting-Started/#62-install-and-use-nginx-version-1158-on-rhel-75",
            "text": "We are using nginx for webserver andalso proxy server for MOSIP project\nCreate the file named /etc/yum.repos.d/nginx.repo using a text editor such as vim command  $ sudo vi /etc/yum.repos.d/nginx.repo  Append following for RHEL 7.5  [nginx]   \nname=nginx repo\nbaseurl=http://nginx.org/packages/mainline/rhel/7/$basearch/\ngpgcheck=0\nenabled=1  After updating repo, please run following commands to install and enable nginx -  $ sudo yum update  $ sudo yum install nginx  $ sudo systemctl enable nginx    To start, stop, restart or get status of nginx use the following commands -  $ sudo systemctl start nginx\n\n$ sudo systemctl stop nginx\n\n$ sudo systemctl restart nginx\n\n$ sudo systemctl status nginx  To edit files use a text editor such as vi  $ sudo vi /etc/nginx/nginx.conf  Example to  configure the nginx for dev environment -\n   ```\n    user  madmin;\n    worker_processes  2;\n    error_log  /var/log/nginx/error.log warn;\n    pid        /var/run/nginx.pid;\n    events {\n        worker_connections  1024;\n    }  http {\n        include       /etc/nginx/mime.types;\n        default_type  application/octet-stream;\n        log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n        '$status $body_bytes_sent \"$http_referer\" '\n        '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n        access_log  /var/log/nginx/access.log  main;\n        client_max_body_size 10m;\n        sendfile        on;\n        tcp_nopush     on;\n        proxy_max_temp_file_size 0;\n        sendfile_max_chunk 10m;\n        keepalive_timeout  65;\n        gzip on;\n        gzip_disable \"msie6\";\n        gzip_vary on;\n        gzip_proxied any;\n        gzip_comp_level 6;\n        gzip_buffers 16 8k;\n        gzip_http_version 1.1;\n        gzip_min_length 256;\n        gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/javascript application/octet-stream application/xml+rss text/javascript application/vnd.ms-fontobject application/x-font-ttf font/opentype image/svg+xml image/x-icon image/png image/jpg;\n        #include /etc/nginx/conf.d/*.conf;",
            "title": "6.2 Install and use Nginx Version-1.15.8 on RHEL 7.5"
        },
        {
            "location": "/Getting-Started/#http-configuration",
            "text": "server {\n        listen  80 default_server;\n        listen [::]:80 default_server ipv6only=on;\n        server_name <your-domain-name>;\n\n        location / {\n                                    root /usr/share/nginx/html;\n                index  index.html index.htm;\n                proxy_http_version 1.1;\n                proxy_set_header Upgrade $http_upgrade;\n                proxy_set_header Connection \"upgrade\";\n                proxy_set_header Host $host;\n                proxy_connect_timeout                   3600s;\n                proxy_send_timeout                      3600s;\n                proxy_read_timeout                      3600s;\n        }\n        return 301 https://$host$request_uri;\n    }",
            "title": "HTTP configuration"
        },
        {
            "location": "/Getting-Started/#https-configuration-for-your-domain",
            "text": "server {\n\n            client_max_body_size 20M;\n                listen *:443 ssl http2;\n                listen [::]:443 ssl http2;\n                server_name dev.mosip.io;\n                ssl on;\n                ssl_certificate         <your-letsencrypt-fullchainpem-path>;\n                ssl_certificate_key   <your-letsencrypt-privatekey-pem-path>;\n        location /v1/keymanager/ {\n                    proxy_set_header Host $host;\n                    proxy_set_header X-Real-IP $remote_addr;\n                    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                    proxy_set_header X-Forwarded-Proto $scheme;\n                    #proxy_set_header Cookie $http_cookie;\n                    proxy_pass  http://<your-keymanager-vm-ip>:<port>/v1/keymanager/;\n            }\n\n             location /registrationprocessor/v1/packetreceiver/ {\n                    proxy_set_header Host $host;\n                    proxy_set_header X-Real-IP $remote_addr;\n                    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                    proxy_set_header X-Forwarded-Proto $scheme;\n                   proxy_pass  http://<your-dmz-vm-ip>:<port>/registrationprocessor/v1/packetreceiver/;\n            }\n\n             location /registrationprocessor/v1/registrationstatus/ {\n                    proxy_set_header Host $host;\n                    proxy_set_header X-Real-IP $remote_addr;\n                    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                    proxy_set_header X-Forwarded-Proto $scheme;\n                    proxy_pass  http://<your-dmz-vm-ip>:<port>/registrationprocessor/v1/registrationstatus/;\n\n            }\n\n            location / {\n                    proxy_set_header Host $host;\n                    proxy_set_header X-Real-IP $remote_addr;\n                    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                    #proxy_set_header X-Forwarded-Host $proxy_add_x_forwarded_for;\n                    proxy_set_header X-Forwarded-Proto $scheme;\n                    proxy_connect_timeout                   3600s;\n                    proxy_send_timeout                      3600s;\n                    proxy_read_timeout                      3600s;\n                    proxy_pass https://<your-dev-k8-cluster-endpoint>; //kubernetes end point\n                    #proxy_intercept_errors on;\n                    #error_page 301 302 307 = @handle_redirects;\n\n            }\n }\n}  ```  Use below command to open the port 80/443 from RHEL 7.5 VM  $ sudo firewall-cmd --zone=public --add-port=80/tcp --permanent\n$ sudo firewall-cmd --zone=public --add-port=443/tcp --permanent\n$ sudo firewall-cmd --reload",
            "title": "HTTPS configuration for your domain"
        },
        {
            "location": "/Getting-Started/#generate-ssltls-for-https-",
            "text": "RHEL 7  version, these are the following commands you have to run to generate certificates for nginx server.    wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n              It will install EPEL script for RHEL 7 OS. It is same as PPA for Ubuntu, it will install some extra packages for enterprise linux edition.\nYou can read more about it https://fedoraproject.org/wiki/EPEL.    This command will run the EPEL install scripts and enable the EPEL packages for RHEL7\n       sudo yum install epel-release-latest-7.noarch.rpm    This will list all the EPEL packages available for used.\n       yum --disablerepo=\"*\" --enablerepo=\"epel\"  list available    Check for python2-certbot-nginx package in EPEL Packages.\n     yum --disablerepo=\"*\" --enablerepo=\"epel\" search python2-certbot-nginx    This will install python certbot for nginx into VM.\n     sudo yum install python2-certbot-nginx    This will generate the certificate for VM.\n     sudo certbot --nginx certonly    Troubleshooting:  If you facing getting this issue in nginx   (13: Permission denied) while connecting to upstream:[nginx]  \nPlease  run below command -  $sudo setsebool -P httpd_can_network_connect 1  or refer link -\nhttps://stackoverflow.com/questions/23948527/13-permission-denied-while-connecting-to-upstreamnginx  Note: Certficates will be generated at, /etc/letsencrypt/live/ / directory. cert.pem is the certificate and privkey.pem is private key. We are using Let's Encrypt, CA signed SSL certificates. Documentation of Let's Encrypt can be referred  here",
            "title": "Generate SSL/TLS for HTTPS -"
        },
        {
            "location": "/Getting-Started/#63-clam-antivirus-version-01010",
            "text": "ClamAV is a free, cross-platform and open-source antivirus software toolkit able to detect many types of malicious software, including viruses.",
            "title": "6.3 Clam AntiVirus Version 0.101.0"
        },
        {
            "location": "/Getting-Started/#steps-to-install-clamav-in-rhel-75",
            "text": "To install clamAV first we need to install EPEL Repository:  $ sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm  After that we need to install ClamAV and its related tools.  $ yum -y install clamav-server clamav-data clamav-update clamav-filesystem clamav clamav-scanner-systemd clamav-devel clamav-lib clamav-server-systemd  After completion of above steps, we need to configure installed ClamAV. This can be done via editing  /etc/clamd.d/scan.conf . In this file we have to remove  Example  lines. So that ClamAV can use this file's configurations. We can easily do it via running following command -  $ sed -i '/^Example/d' /etc/clamd.d/scan.conf  Another thing we need to do in this file is to define our TCP server type. Open this file using -  $ vim /etc/clamd.d/scan.conf  here this we need to uncomment line with  #LocalSocket /var/run/clamd.scan/clamd.sock . Just remove  #  symbol from the beginning of the line.  Now we need to configure FreshClam so that it can update ClamAV db automatically. For doing that follow below steps -  First create a backup of original FreshClam Configuration file -  $ cp /etc/freshclam.conf /etc/freshclam.conf.bak  In this  freshclam.conf  file, Here also we need to remove  Example  line from the file. Run following command to delete all  Example  lines-  $ sed -i '/^Example/d' /etc/freshclam.conf  Test freshclam via running-  $ freshclam  After running above command you should see an output similar to this -  ClamAV update process started at Thu May 23 07:25:44 2019\n.\n.\n.\n.\nmain.cvd is up to date (version: 58, sigs: 4566249, f-level: 60, builder: sigmgr)\nDownloading daily-25584.cdiff [100%]\ndaily.cld updated (version: 25584, sigs: 1779512, f-level: 63, builder: raynman)\nbytecode.cld is up to date (version: 331, sigs: 94, f-level: 63, builder: anvilleg)\nDatabase updated (6345855 signatures) from database.clamav.net (IP: 104.16.218.84)  We will create a service of freshclam so that freshclam will run in the daemon mode and periodically check for updates throughout the day. To do that we will create a service file for freshclam -  $ vim /usr/lib/systemd/system/clam-freshclam.service  And add below content -  [Unit]\nDescription = freshclam scanner\nAfter = network.target\n\n[Service]\nType = forking\nExecStart = /usr/bin/freshclam -d -c 4\nRestart = on-failure\nPrivateTmp = true\nRestartSec = 20sec\n\n[Install]\nWantedBy=multi-user.target  Now save and quit. Also reload the systemd daemon to refresh the changes -  $ systemctl daemon-reload  Next start and enable the freshclam service -  $ systemctl start clam-freshclam.service\n\n$ systemctl enable clam-freshclam.service  Now freshclam setup is complete and our ClamAV db is upto date. We can continue setting up ClamAV. Now we will copy ClamAV service file to system service folder.  $ mv /usr/lib/systemd/system/clamd@.service /usr/lib/systemd/system/clamd.service  Since we have changed the name, we need to change it at the file that uses this service as well -  $ vim /usr/lib/systemd/system/clamd@scan.service  Remove @ symbol from  .include /lib/systemd/system/clamd@.service  line and save the file.  We will edit Clamd service file now -  $ vim /usr/lib/systemd/system/clamd.service  Add following lines at the end of clamd.service file.  [Install]\nWantedBy=multi-user.target  And also remove  %i  symbol from various locations (ex: Description and ExecStart options). Note that at the end of the editing the service file should look something like this -  [Unit]\nDescription = clamd scanner daemon\nDocumentation=man:clamd(8) man:clamd.conf(5) https://www.clamav.net/documents/\n# Check for database existence\n# ConditionPathExistsGlob=@DBDIR@/main.{c[vl]d,inc}\n# ConditionPathExistsGlob=@DBDIR@/daily.{c[vl]d,inc}\nAfter = syslog.target nss-lookup.target network.target\n\n[Service]\nType = forking\nExecStart = /usr/sbin/clamd -c /etc/clamd.d/scan.conf\nRestart = on-failure\n\n[Install]\nWantedBy=multi-user.target  Now finally start the ClamAV service.  $ systemctl start clamd.service  If it works fine, then enable this service and test the status of ClamAV service -  $ systemctl enable clamd.service\n\n$ systemctl status clamd.service  Now in MOSIP we require ClamAV to be available on Port 3310. To expose ClamAV service on Port 3310, edit  scan.conf  $ vi /etc/clamd.d/scan.conf  and Uncomment  #TCPSocket 3310  by removing  # . After that restart the clamd@scan service -  $ systemctl restart clamd@scan.service  Since we are exposing ClamAV on 3310 port, we need to allow incoming traffic through this port. In RHEL 7 run below command to add firewall rule -  $ sudo firewall-cmd --zone=public --add-port=3310/tcp --permanent\n$ sudo firewall-cmd --reload",
            "title": "Steps to install ClamAV in RHEL-7.5"
        },
        {
            "location": "/Getting-Started/#reference-link-link",
            "text": "",
            "title": "Reference link: link"
        },
        {
            "location": "/Getting-Started/#64-steps-to-install-and-configuration-ceph",
            "text": "NOTE: Required only if CEPH is used for packet storage.\nCeph is an open source software that provides massively scalable and distributed data store. It provides highly scalable object, block and file based storage under a unified system.",
            "title": "6.4 Steps to Install and configuration CEPH"
        },
        {
            "location": "/Getting-Started/#1-on-red-hat-enterprise-linux-7-register-the-target-machine-with-subscription-manager-verify-your-subscriptions-and-enable-the-extras-repository-for-package-dependencies-for-example",
            "text": "$ sudo subscription-manager repos --enable=rhel-7-server-extras-rpms",
            "title": "1. On Red Hat Enterprise Linux 7, register the target machine with subscription-manager, verify your subscriptions, and enable the \u201cExtras\u201d repository for package dependencies. For example:"
        },
        {
            "location": "/Getting-Started/#2-install-and-enable-the-extra-packages-for-enterprise-linux-epel-repository",
            "text": "$ sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm",
            "title": "2. Install and enable the Extra Packages for Enterprise Linux (EPEL) repository:"
        },
        {
            "location": "/Getting-Started/#3-add-the-ceph-repository-to-your-yum-configuration-file-at-etcyumreposdcephrepo-with-the-following-command-replace-ceph-stable-release-with-a-stable-ceph-release-eg-luminous-for-example",
            "text": "cat << EOM > /etc/yum.repos.d/ceph.repo\n[ceph-noarch]\nname=Ceph noarch packages\nbaseurl=https://download.ceph.com/rpm-{ceph-stable-release}/el7/noarch\nenabled=1  \ngpgcheck=1\ntype=rpm-md\ngpgkey=https://download.ceph.com/keys/release.asc\nEOM",
            "title": "3. Add the Ceph repository to your yum configuration file at /etc/yum.repos.d/ceph.repo with the following command.  Replace {ceph-stable-release} with a stable Ceph release (e.g., luminous.) For example:"
        },
        {
            "location": "/Getting-Started/#4-update-your-repository-and-install-ceph-deploy",
            "text": "$ sudo yum update\n$ sudo yum install ceph-deploy",
            "title": "4. Update your repository and install ceph-deploy:"
        },
        {
            "location": "/Getting-Started/#ceph-node-setup",
            "text": "The admin node must have password-less SSH access to Ceph nodes. When ceph-deploy logs in to a Ceph node as a user, that particular user must have passwordless sudo privileges.",
            "title": "CEPH NODE SETUP"
        },
        {
            "location": "/Getting-Started/#install-ntp",
            "text": "We recommend installing NTP on Ceph nodes (especially on Ceph Monitor nodes) to prevent issues arising from clock drift. See  [Clock](//docs.ceph.com/docs/mimic/rados/configuration/mon-config-ref/#clock) for details.\n$ sudo yum install ntp ntpdate ntp-doc\n    Ensure that you enable the NTP service. Ensure that each Ceph Node uses the same NTP time server.",
            "title": "INSTALL NTP"
        },
        {
            "location": "/Getting-Started/#install-ssh-server",
            "text": "sudo yum install openssh-server\n    Ensure the SSH server is running on ALL Ceph Nodes.  1) Make a directory on admin node in order to keep all the keys and configuration files that ceph-deploy generates.\n    a. mkdir cluster-config\n    b. cd cluster-config\n2) Now we create a cluster  ceph-deploy new {initial-monitor-node(s)}\n\nceph-deploy new ceph-demo-1 ceph-demo-2",
            "title": "INSTALL SSH SERVER"
        },
        {
            "location": "/Getting-Started/#this-step-marks-the-nodes-as-initial-monitors",
            "text": "3) Thereafter, we Install ceph packages on required nodes  ceph-deploy install {ceph-node} [\u2026]\n\nceph-deploy install ceph-demo-1 ceph-demo-2",
            "title": "This step marks the nodes as initial monitors."
        },
        {
            "location": "/Getting-Started/#this-step-will-install-the-latest-stable-version-of-ceph-ie-mimic-1321-on-the-given-nodes",
            "text": "4) Now, we deploy the initial monitor nodes and gather keys  ceph-deploy mon create-initial  5) Now we go ahead and copy our config file and admin key to the admin node as well as ceph-nodes in order to use ceph cli without passing these each time we execute a command.  ceph-deploy admin {ceph-node}[\u2026]\n\nceph-deploy admin ceph-demo-1 ceph-demo-2  6) Now, we deploy a manager daemon  ceph-deploy mgr create {ceph-node}[\u2026]\n\nceph-deploy mgr create ceph-demo-1 ceph-demo-2  7) We create 2 OSDs, assuming each osd has a unused disk called dev/sdb  ceph-deploy osd create\u200a\u2014\u200adata {device} {ceph-node}\n\nceph-deploy osd create \u2014 data /dev/sdb ceph-demo-1\nceph-deploy osd create \u2014 data /dev/sdb ceph-demo-2",
            "title": "This step will install the latest stable version of ceph, i.e. mimic (13.2.1) on the given nodes."
        },
        {
            "location": "/Getting-Started/#after-successfully-executing-these-steps-our-ceph-cluster-is-up-and-running-the-status-and-health-of-the-cluster-can-be-checked-in-by-executing",
            "text": "$ sudo ceph health\n$ sudo ceph -s",
            "title": "After successfully executing these steps, our ceph cluster is up and running. The status and health of the cluster can be checked in by executing"
        },
        {
            "location": "/Getting-Started/#we-should-get-a-status-saying-health_ok-and-a-detailed-status-resembling",
            "text": "cluster:\n\nid: 651e9802-b3f0\u20134b1d-a4d6-c57a46635bc9\n\nhealth: HEALTH_OK\n\nservices:\n\nmon: 2 daemons, quorum ceph-demo-1,ceph-demo-2\n\nmgr: ceph-demo-1(active), standbys: ceph-demo-2\n\nosd: 2 osds: 2 up, 2 in\n\ndata:\n\npools: 0 pools, 0 pgs\n\nobjects: 0 objects, 0 B\n\nusage: 2.0 GiB used, 18 GiB / 20 GiB avail\n\npgs:",
            "title": "We should get a status saying HEALTH_OK, and a detailed status resembling :"
        },
        {
            "location": "/Getting-Started/#expanding-the-existing-cluster",
            "text": "Now, to demonstrate the ease of expanding a ceph cluster at runtime, we will be adding one node in our running cluster. We will mark that node as osd, manager and monitor to increase the availability of our existing cluster.\nFirst of all, we need to make a change to our existing ceph.conf which is present inside the cluster-config directory. We add the following line into it",
            "title": "Expanding the Existing cluster"
        },
        {
            "location": "/Getting-Started/#public-network-ip-addressbits",
            "text": "public network = 10.142.0.0/24",
            "title": "public network = {ip-address}/{bits}"
        },
        {
            "location": "/Getting-Started/#for-this-we-need-to-follow-these-sample-steps",
            "text": "1) We install ceph packages on 3rd node  ceph-deploy install {ceph-node}[\u2026]\n\nceph-deploy install ceph-demo-3  2) We need to push the admin keys and conf to 3rd node. We do it using  ceph-deploy admin {ceph-node}[\u2026]\n\nceph-deploy admin ceph-demo-3  3) Now we will add the 3rd node as our monitor  ceph-deploy mon add {ceph-nodes}\n\nceph-deploy mon add ceph-demo-3  4) Now, we go ahead and mark 3rd node as our manager  ceph-deploy mgr create {ceph-node}[\u2026]\n\nceph-deploy mgr create ceph-demo-3  5) We add 3rd node as OSD by following same steps as done while creating the cluster.  ceph-deploy osd create\u200a\u2014\u200adata {path} {ceph-node}",
            "title": "For this, we need to follow these sample steps:"
        },
        {
            "location": "/Getting-Started/#ceph-dashboard",
            "text": "Now, going ahead, we can enable the CEPH dashboard in order to view all the cluster status via a UI console.  Ceph in its mimic release has provided the users with a new and redesigned dashboard plugin, with the features like restricted control with username/password protection and SSL/TLS support.",
            "title": "CEPH Dashboard"
        },
        {
            "location": "/Getting-Started/#to-enable-the-dashboard-we-need-to-follow-these-steps",
            "text": "1) ceph mgr module enable dashboard\n\n2) ceph dashboard create-self-signed-cert  Note: Self signed certificate is only for quick start purpose.  3) ceph mgr module disable dashboard\n\n4) ceph mgr module enable dashboard  Now, we will be able to see the CEPH dashboard on the port 8443, which is by default but on requirement can be configured using:  ceph config set mgr mgr/dashboard/server_addr $IP\n\nceph config set mgr mgr/dashboard/server_port $PORT  To access, and to utilize full functionality of the dashboard, we need to create the login credentials.  ceph dashboard set-login-credentials <username> <password>  After these steps, our ceph infrastructure is ready with all the configurations to do some actual input output operations.",
            "title": "To enable the dashboard, we need to follow these steps:"
        },
        {
            "location": "/Getting-Started/#reference-link",
            "text": "http://docs.ceph.com/docs/mimic/start/quick-start-preflight/",
            "title": "Reference link:"
        },
        {
            "location": "/Getting-Started/#65-steps-to-install-and-configuration-ldap",
            "text": "ApacheDs Server installation and config  Apache Directory Studio user guide",
            "title": "6.5 Steps to Install and configuration LDAP"
        },
        {
            "location": "/Getting-Started/#66-steps-to-install-and-configuration-hdfs",
            "text": "NOTE: Required only if HDFS is used for packet storage.  Refer - Steps-to-Install-and-configuration-HDFS",
            "title": "6.6 Steps to Install and configuration HDFS"
        },
        {
            "location": "/Getting-Started/#67-steps-to-deploy-kernel-key-manager-service",
            "text": "Kernel Keymanager Service is setup outside of Kubernetes cluster on a standalone VM. The steps to setup kernel-keymanager-service are given  here  To deploy keymanager service, follow below steps -\n1.  Prerequiste: \n       *  A machine with RHEL 7.6 installed.\n       * Docker installed and Docker service enabled.  Steps to install Docker ce.   $ sudo yum install http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.107-3.el7.noarch.rpm\n\n $ sudo yum -y install lvm2 device-mapper device-mapper-persistent-data device-mapper-event device-mapper-libs    device-mapper-event-libs\n\n$ sudo wget https://download.docker.com/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\n\n$ sudo yum -y install docker-ce\n\n$ sudo systemctl start docker\n\n$ sudo systemctl status docker   Open port 8088 on the VM:   sudo firewall-cmd --zone=public --add-port=8088/tcp --permanent\n\nsudo firewall-cmd --reload  Note:  if firewall is not installed in VM, install with \u201csudo yum install firewall\u201d  And also open the port.   ensure that config server is already deployed.   Process to deploy Services in VM through JenkinsFile:   Refer the github url for Jenkinsfile : in root directory of kernel module   The last stage in the Jenkinsfile viz 'Key-Manager Deployment' in which we are sshing into this newly created VM through Jenkins to deploy this service, basically, running the docker image of key manager.  For ssh, place the public key of jenkins inside this newly created VM's authorized_keys under .ssh directory. Generate Docker Registry Credential in jenkins by using docker hub username and password. This will generate the credentialsId.    Replace the value for registryCredentials(credentialsId of docker hub) with yours    Replace the value for  key_manager_vm_ip with IP of your newly created VM.    Once done the following command will be used to deploy keymanager to the machine:   sudo docker run -tid --ulimit memlock=-1 -p 8088:8088 -v softhsm:/softhsm -e spring_config_url_env=\"${config_url}\" -e spring_config_label_env=\"${branch}\" -e active_profile_env=\"${profile_env}\" --name keymanager \"${registryAddress}\"/kernel-keymanager-service  **NOTE- Replace the values for spring_config_url_env, spring_config_label_env,\n      active_profile_env and registryAddress in the above command accordingly",
            "title": "6.7 Steps to Deploy Kernel Key Manager Service"
        },
        {
            "location": "/Getting-Started/#68-sms-gateway-configuration",
            "text": "Refer kernel-smsnotification-servive Readme  here",
            "title": "6.8 SMS Gateway configuration"
        },
        {
            "location": "/Getting-Started/#69-installation-of-activemq",
            "text": "ActiveMQ is the message broker used for MOSIP Registartion processor module.",
            "title": "6.9 Installation of ActiveMQ"
        },
        {
            "location": "/Getting-Started/#installation-steps",
            "text": "<version>  : please check http://www.apache.org/dist/activemq/ to find out the latest version. Tested ActiveMQ version - 5.4.1.  Prerequiste: \n        A machine with RHEL 7.6 installed, Docker installed and Docker service enabled.  Download activemq using command -   wget https://archive.apache.org/dist/activemq/5.14.3/apache-activemq-5.14.3-bin.tar.gz  Extract the archive   tar -zxvf apache-activemq-<version>-bin.tar.gz  Change the permission for startup script  chmod 755 apache-activemq-<version>  Start activemq service  cd apache-activemq-<version> && sudo  ./bin/activemq start  Check for the installed and started activemq on port 61616.   netstat -tulpn  Open ports 8161 and 61616  on the VM:   sudo firewall-cmd --zone=public --add-port=8161/tcp --permanent\nsudo firewall-cmd --zone=public --add-port=61616/tcp --permanent\nsudo firewall-cmd --reload  Note:  After Installation of activemq, same needs to be mentioned in RegistrationProcessorAbis_{active_profile}.json\nFor e.g : Suppose activemq is configured as tcp://xxx.xxx.xxx.xx:61616, then we for dev need to mention this in RegistrationProcessorAbis_dev.json as  {\n    \"abis\": [{\n            \"name\": \"ABIS1\",\n            \"host\": \"\",\n            \"port\": \"\",\n            \"brokerUrl\": \"tcp://xxx.xxx.xxx.xx:61616\",\n            \"inboundQueueName\": \"abis1-inbound-address_dev\",\n            \"outboundQueueName\": \"abis1-outbound-address_dev\",\n            \"pingInboundQueueName\": \"\",\n            \"pingOutboundQueueName\": \"\",\n            \"userName\": \"admin\",\n            \"password\": \"admin\",\n                \"typeOfQueue\": \"ACTIVEMQ\"\n        }\n    ]\n\n}  ActiveMQ is also being used in registration-processor-printing-stage and the details need to be mentioned in registration-processor-{active_profile}.properties in the configuration repository.\nE.g : For dev profile, the property in registration-processor-dev.properties, the Property corresponding to printing-stage related to activemq would be  Queue username\nregistration.processor.queue.username={username}\n#Queue Password\nregistration.processor.queue.password={password}\n#Queue Url\nregistration.processor.queue.url={queue_url}\n#Type of the Queue\nregistration.processor.queue.typeOfQueue=ACTIVEMQ\n#Print Service address\nregistration.processor.queue.address={queue_address}\n#Post Service address\nregistration.processor.queue.printpostaladdress={postal_queue_address}",
            "title": "Installation steps"
        },
        {
            "location": "/Getting-Started/#7-configuring-mosip",
            "text": "",
            "title": "7. Configuring MOSIP [\u2191]"
        },
        {
            "location": "/Getting-Started/#mosip-database-object-deployment-configuration",
            "text": "Database deployment consists of the following 4 categories of objects to be deployed on postgresql database.    User / Roles:  In MOSIP, the following user / roles are defined to perform various activities    sysadmin:  sysadmin user/role is a super administrator role, who will have all the privileges to performa any task within the database.    dbadmin:  dbadmin user / role is created to handle all the database administration activities db monitoring, performance tuning, backups, restore, replication setup, etc.    appadmin:  appadmin user / role is used to perform all the DDL (Data Definition Language) tasks. All the db objects that are created in these databases will be owned by appadmin user.    Application User:  Each application will have a user / role created to perform DML (Data Manipulation Language) tasks like CRUD operations (select, insert, update, delete). The user prereguser, is created to connect from the application to perform all the DML activities. Similarly, we will have masteruser, prereguser, reguser, idauser, idrepouser, kerneluser, audituser, regprcuser to perform DML tasks for master, pre-registration, registration, ida, ID repository, kernel, audit and registration processor modules respectively.    Note:  From the above set of roles only application user / role is specific to a application / module. The other user / roles are common which needs to be  created per postresql db instance / server.    Database and Schema:  Each application / module of MOSIP platform will have a database and schema defined. All the objects (tables) related to an application / module would be created under the respective database / schema. In MOSIP the following database and scehmas are defined       application / module name  Database tool  database Name  schema name      Master / Administration module  postgresql  mosip_master  master    Kernel  postgresql  mosip_kernel  kernel    Pre-registration  postgresql  mosip_prereg  prereg    Registration  Apache Derby  mosip_reg  reg    Registration Processor  postgresql  mosip_regprc  regprc    ID Authentication  postgresql  mosip_ida  ida    ID Repository  postgresql  mosip_idrepo  idrepo    Audit  postgresql  mosip_audit  audit    IAM  postgresql  mosip_iam  iam    idmap  postgresql  mosip_idmap  idmap     Note:  These databases can be deployed on single or separate database servers / instances.    DB Objects (Tables):  All the tables of each application / module will be created in their respective database and schema. appadmin user / role will own these objects and the respective application user / role will have access to perform DML operations on these objects.    Seed Data:  MOSIP platform is designed to provide most of its features to be configured in the system. These configuration are deployed with default setup on config server and few in database. Few of these configuration can be modified / updated by the MOSIP administrator. These configuration include, system configurations, master datasetup, etc. The steps to add new center, machine / device is detailed in  Guidelines for Adding Centers, Machine, Users and Devices    The system configuration and master data is available under the respective application / database related folder. for example, the master data configuration is available in csv file format under  folder .  The scripts to create the above objects are available under  db_scripts . To deploy the database objects of each application / module  except registration client , please refer to  README.MD  file. These scripts will contain the deployment of all the DB object categories.  Note: Please skip Registration client related deployment scripts (Apache derby DB specific) as this will be executed as part of registration client software installation.",
            "title": "MOSIP database object deployment / configuration"
        },
        {
            "location": "/Getting-Started/#setup-and-configure-mosip",
            "text": "We are using kubernetes configuration server in MOSIP for storing and serving distributed configurations across all the applications and environments.  We are storing all applications' configuration in config-templates folder inside our Github Repository  here .  For getting more details about how to use configuration server with our applications, following developer document can be referred: MOSIP CONFIGURATION SERVER  For Deployment of configurations server, go to  firstly-deploy-kernel-configuration-server  in this document.  Application specific configuration for all applications and services are placed in MOSIP config server.  A. Global:  link  B. Kernel:  link  C. Pre-Registration:  link  D. Registartion-Processor:  link  E. IDA:  link  F. ID-REPO:  link  H. Registration:  link  Properties Sections that need to be changed in above module specific files once the external dependencies are installed as per your setup",
            "title": "Setup and configure MOSIP"
        },
        {
            "location": "/Getting-Started/#global",
            "text": "",
            "title": "Global"
        },
        {
            "location": "/Getting-Started/#-common-properties-",
            "text": "mosip.base.url",
            "title": "--Common properties------------"
        },
        {
            "location": "/Getting-Started/#-virus-scanner-",
            "text": "mosip.kernel.virus-scanner.host\nmosip.kernel.virus-scanner.port",
            "title": "--Virus Scanner-----------------"
        },
        {
            "location": "/Getting-Started/#-fs-adapter-hdfs-",
            "text": "mosip.kernel.fsadapter.hdfs.name-node-url\n# Enable if hadoop security authorization is 'true', default is false\nmosip.kernel.fsadapter.hdfs.authentication-enabled\n# If HDFS is security is configured with Kerberos, Key Distribution Center domain\nmosip.kernel.fsadapter.hdfs.kdc-domain\n#keytab file path, must be set if authentication-enable is true\n#read keytab file both classpath and physical path ,append appropriate prefix\n#for classpath prefix classpath:mosip.keytab\n#for physical path prefix file:/home/keys/mosip.keytab\nmosip.kernel.fsadapter.hdfs.keytab-file=classpath:mosip.keytab",
            "title": "--FS Adapter-HDFS -------------"
        },
        {
            "location": "/Getting-Started/#kernel",
            "text": "",
            "title": "Kernel"
        },
        {
            "location": "/Getting-Started/#-kernel-common-properties-",
            "text": "mosip.kernel.database.hostname\nmosip.kernel.database.port",
            "title": "--kernel common properties-----------------------"
        },
        {
            "location": "/Getting-Started/#-sms-notification-service-",
            "text": "mosip.kernel.sms.authkey",
            "title": "--sms notification service-----------------------"
        },
        {
            "location": "/Getting-Started/#-email-notification-service-",
            "text": "spring.mail.host\nspring.mail.username\nspring.mail.password\nspring.mail.port=587",
            "title": "--Email Notification service---------------------"
        },
        {
            "location": "/Getting-Started/#-ldap-",
            "text": "ldap_1_DS.datastore.ipaddress\nldap_1_DS.datastore.port",
            "title": "--Ldap------------"
        },
        {
            "location": "/Getting-Started/#-database-properties-",
            "text": "**_database_password\n**_database_username",
            "title": "--DataBase Properties----------------------------"
        },
        {
            "location": "/Getting-Started/#8-mosip-deployment",
            "text": "Currently for the Development Process MOSIP Platform is deployed as/in the Kubernetes Cluster. We are using Azure Kubernetes Service for provisioning of Cluster. As of now Kubernetes Deployment is deviced in two parts -  A. One time setup of MOSIP in Kubernetes Cluster  B. Continuous deployment",
            "title": "8. MOSIP Deployment [\u2191]"
        },
        {
            "location": "/Getting-Started/#a-one-time-setup-of-mosip-in-kubernetes-cluster",
            "text": "One time setup on Kubernetes involves following Steps  \nI. Setting Up local system to communicate with Kubernetes cluster this can be done via  kubectl . \nII. Setting Up the Basic environment for MOSIP to run in Kubernetes Cluster, In this step we will work on this  link  . following are the files -   We will now go through each of the file and see what changes we need to perform. we will be using  kubectl  to do the deployments from local system.   DeployIngressController.yaml - We need not to change anything here. we can directly run this file. To run this use this command kubectl apply -f DeployIngressController.yaml   DeployIngress.yaml -\nThis file contains information about routing to different Kubernetes services, So whenever any traffic comes to our Load Balancer IP it will look for this file to route the request. For eg. Let's say if  some.example.com  is mapped to our kubernetes loadbalancer then if a request is for  some.example.com/pre-registration-ui  then this request will be redirect to  pre-registration-ui  on port  80  service. Routes referrring to  ping-server  and  sample-nginx  can be removed as these are for testing purpose.To run this use this command kubectl apply -f DeployIngress.yaml    DeployServiceIngressService.yaml - We need not to change anything here. we can directly run this file. To run this use this command\nkubectl apply -f DeployServiceIngressService.yaml    DeployDefaultBackend.yaml - We need not to change anything here. we can directly run this file. To run this use this command kubectl apply -f DeployDefaultBackend.yaml    docker-registry-secret.yml -\nThis file helps Kubernetes to get the Docker Images from Private Docker Registry. This file is a downloaded YAML of secrets that exists in the Kubernetes. You can either create secret or use this file to deploy secret in Kubernetes. For creating secret for the first time, run below command -    kubectl create secret docker-registry <registry-credential-name> --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>  Once secret is created on the Kubernetes Cluster, as a backup strategy we can download the created secret using this command  kubectl get secret <registry-credential-name> -o yaml --export  Once the above deployment is done, we will start deploying MOSIP services. For doing this, we need to look for these directories -",
            "title": "A. One time setup of MOSIP in Kubernetes Cluster"
        },
        {
            "location": "/Getting-Started/#firstly-deploy-kernel-configuration-server",
            "text": "The script is inside ( https://github.com/mosip/mosip-infra/blob/master/deployment/cloud/kubernetes/configuration-server/config-server-deployment-and-service.yml )  \nFollow below steps:\n1. Create a ssh key and configure it with your git repository. If you have already configured the ssh key for your repository, you can use that one or else follow  this   \n2. Create a secret for Config server to connect to GIT repo. This secret contains your  id_rsa key (private key), id_rsa_pub key (public key) and known_hosts  which you generated above. We need this secret because config server connects to your Source code management repository, to get configuration for all the services(If you are using ssh URL for cloning the repo). For generating the required secret give the following command: ( Firstly try to connect to GIT repository from your system using ssh url and the key you created above, so that GIT service provider such as GitHub or GitLab comes in your known hosts file):   `kubectl create secret generic config-server-secret --from-file=id_rsa=/path/to/.ssh/id_rsa --from-file=id_rsa.pub=/path/to/.ssh/id_rsa.pub --from-file=known_hosts=/path/to/.ssh/known_hosts` <br/>\n\n**For Encryption Decryption of properties with configuration server** <br/>\n<br/>\nCreate keystore with following command:\n\n`keytool -genkeypair -alias <your-alias> -keyalg RSA -keystore server.keystore -keypass < key-secret > -storepass < store-password > --dname \"CN=<your-CN>,OU=<OU>,O=<O>,L=<L>,S=<S>,C=<C>\"`   The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format, migrate it using following command:  keytool -importkeystore -srckeystore server.keystore -destkeystore server.keystore -deststoretype pkcs12   \nFor more information look  here      Create file with following content to create keystore secret for encryption decryption of keys using information from keystore created above:   apiVersion: v1\nkind: Secret\nmetadata:\n  name: config-server-keystore-values-secret\ntype: Opaque\ndata:\n  alias: < base-64-encoded-alias-for keystore >\n  password: <  base-64-store-password >\n  secret: < base-64-encoded-key-secret > \n5. Save the above file with any name and apply it using:   kubectl apply -f < file-name >     Create server.keystore as secret to volume mount it inside container:   kubectl create secret generic config-server-keystore --from-file=server.keystore=< location-of-your-server.keystore-file-generated-above >     Change  git_url_env  environment variable in kernel-config-server-deployment-and-service.yml to your git ssh url of configuration repository   Change  git_config_folder_env  environment variable in kernel-config-server-deployment-and-service.yml  to your configuration folder in git repository.   Change  spec->template->spec->containers->image  from  docker-registry.mosip.io:5000/kernel-config-server  to  < Your Docker Registry >/kernel-config-server     Change  spec->template->spec->imagePullSecrets->name  from  pvt-reg-cred  to  < Your docker registry credentials secret >    Once above configuration is done, execute  kubectl apply -f kernel-config-server-deployment-and-service.yml     More information can be found  here",
            "title": "Firstly Deploy Kernel Configuration server"
        },
        {
            "location": "/Getting-Started/#deploy-other-components",
            "text": "Inside each of the directory there is a file for each service of MOSIP that is exposed as Web API. We need to deploy these files to get these running. But before doing that we need to change Private Docker Registry Address and Docker Registry Secret, so that on deployment time Kubernetes can fetch docker images from correct source using correct credentials.\nFor doing this, follow below steps (for eg. we will use https://github.com/mosip/mosip-infra/blob/master/deployment/cloud/kubernetes/kernel-deployment/kernel-auditmanager-service-deployment-and-service.yml, but you have to repeat the process for all such files) -  \nI. Open a deployment file.  \nII. Change  spec->template->spec->containers->image  from  docker-registry.mosip.io:5000/kernel-auditmanager-service  to  <Your Docker Registry>/kernel-auditmanager-service   \nIII. Change  spec->template->spec->imagePullSecrets->name  from  pvt-reg-cred  to  <Your docker registry credentials secret>   \nIV. Change  active_profile_env  to whichever profile you want to activate and  spring_config_label_env  to the branch from which you want to pick up the configuration \nV. Save the file and Run  kubectl apply -f kernel-auditmanager-service-deployment-and-service.yml    After above process is completed, you can run  kubectl get services  command to see the status of all the MOSIP Services.  For Pre-Registration-UI \nPre-registration-ui uses a file config.json to configure URLs of backend, which have to be provided as config map in pre-registration-ui-deployment-and-service.yml. For creating the configmap follow below steps:\n1. Edit the file scripts -> https://github.com/mosip/mosip-infra/blob/master/deployment/cloud/kubernetes/pre-registration-deployment/pre-registration-ui-configuration.yaml\n2. Update  https://dev.mosip.io/  value with url of proxy server which points to pre-registration services. (Note: While editing, be careful with escape sequence characters)\n3. Execute command  Kubectl apply -f pre-registration-ui-configuration.yaml",
            "title": "Deploy other components: "
        },
        {
            "location": "/Getting-Started/#81-registration-processor-dmz-services-deployment",
            "text": "Registration Processor DMZ Services are setup externally(deployed in a separate VM).  Firstly, update below files present in config folder in configuration repository, and replace the line \n  <to uri=\"https://<dns name>/registrationprocessor/v1/uploader/securezone\" /> \n with the URL of packet uploader stage. \n 1.   registration-processor-camel-routes-new-dmz-<env-name>.xml \n 2.   registration-processor-camel-routes-update-dmz-<env-name>.xml \n 3.   registration-processor-camel-routes-lost-dmz-<env-name>.xml \n 4.   registration-processor-camel-routes-activate-dmz-<env-name>.xml \n 5.   registration-processor-camel-routes-deactivate-dmz-<env-name>.xml \n 6.   registration-processor-camel-routes-res_update-dmz-<env-name>.xml  We are deploying DMZ services into another VM having docker installed. The steps to setup DMZ environment and services deployment:\n1. Need to set Up VM with RHEL 7.6\n2. Installing the Docker ce:        $ sudo yum install http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.107-\n        3.el7.noarch.rpm\n\n     $ sudo yum -y install lvm2 device-mapper device-mapper-persistent-data device-mapper-event device-mapper-libs\n       device-mapper-event-libs\n\n     $ sudo wget https://download.docker.com/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\n\n     $ sudo yum -y install docker-ce\n\n     $ sudo systemctl start docker\n\n     $ sudo systemctl status docker   Need to copy the Jenkins server public key(id_rsa.pub) inside this newly created VM's authorized_keys(because through jenkins job, we will ssh into new VM and deploy)   After installing Docker Start the Docker Service  command to start the Docker service   systemctl start docker   command to check Docker is running:    systemctl status docker    Open the port 8081, 8083 from the VM: \nMosip uses port 8081 for registration-processor-packet-receiver-stage and 8083 for registration-processor-registration-status-service. The port ids need to be updated in ngnix configuration.    sudo firewall-cmd --zone=public --add-port=8081/tcp --permanent  sudo firewall-cmd --reload  sudo firewall-cmd --zone=public --add-port=8083/tcp --permanent  sudo firewall-cmd --reload  Note:  if firewall is not installed in VM, install with \u201csudo yum install firewall\u201d  And also open the port from AZURE OR AWS or any cloud where the VM is launched.  Process to deploy Services in VM through JenkinsFile:   The last stage in the Jenkinsfile viz DMZ_Deployment in which we are sshing into this newly created VM through Jenkins to deploy these services, basically, running the docker images of registration processor.\nChanges to be made in this stage->   a. Replace the value for registryCredentials(credentialsId of docker hub) with yours.  b. Replace the value for the variable -> dmz_reg_proc_dev_ip with the IP of your newly created VM.  Refer the github url for Jenkinsfile :  here   Also, instead of following as described in 4th point to use Jenkinsfile, we can do it manually. Steps are ->   a. Login into the DMZ VM.  b. Perform docker hub login  c. Execute the following commands  docker run --restart always -it -d -p 8083:8083 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" \"${registryAddress}\"/registration-processor-registration-status-service  docker run --restart always -it -d --network host --privileged=true -v /home/ftp1/LANDING_ZONE:/home/ftp1/LANDING_ZONE -v /home/ftp1/ARCHIVE_PACKET_LOCATION:/home/ftp1/ARCHIVE_PACKET_LOCATION -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" \"${registryAddress}\"/registration-processor-packet-receiver-stage  docker run --restart always -it -d --network host --privileged=true -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e zone_env=dmz  \"${registryAddress}\"/registration-processor-common-camel-bridge  Note  - Please change the environmental variables(active_profile_env, spring_config_label_env, spring_config_url_env ,registryAddress) in the above commands accordingly whether you are executing manually in your new VM or through Jenkinsfile.   Packet uploader stage in secure zone will fetch file from dmz to upload it into Distributed File System,to connect to\n   dmz vm either we can login using username and password or using ppk file.\n   If password value is available in config property name\n   registration.processor.dmz.server.password then uploader will connect using username and password.\n   otherwise it will login using ppk file available in config with property name registration.processor.vm.ppk.\n   PPK generation command ssh-keygen -t rsa -b 4096 -f mykey.",
            "title": "8.1  Registration-Processor DMZ services deployment"
        },
        {
            "location": "/Getting-Started/#82-kernel-salt-generator",
            "text": "Kernel Salt Generator Job is a one-time job which is run to populate salts to be used to hash and encrypt data. This generic job takes schema and table name as input, and generates and populates salts in the given schema and table.  Salt Generator Deployment steps  a. Login into the VM.\n     Open the port 8092 from the VM:  sudo firewall-cmd --zone=public --add-port=8092/tcp --permanent  sudo firewall-cmd --reload  And also open the port from AZURE OR AWS or any cloud where the VM is launched.  b. Perform docker hub login  c. Execute the following commands sequentially one after the other. Wait for the completion of previous command before\n     executing next commands.    1.  docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\"  \n     -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.identity.db.shard.url -e schema_name=idrepo -e table_name=uin_hash_salt\n     \"${registryAddress}\"/id-repository-salt-generator\n\n  2.  docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\"  \n     -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.identity.db.shard.url -e schema_name=idrepo -e table_name=uin_encrypt_salt\n     \"${registryAddress}\"/id-repository-salt-generator\n\n  3.  docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\"  \n     -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.vid.db.url -e schema_name=idmap -e table_name=uin_hash_salt\n     \"${registryAddress}\"/id-repository-salt-generator\n\n  4.  docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\"       \n     -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.vid.db.url -e schema_name=idmap -e table_name=uin_encrypt_salt\n     \"${registryAddress}\"/id-repository-salt-generator\n\n  5.  docker run -it -d -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\"\n      -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-authentication -e schema_name=ida -e table_name=uin_hash_salt\n      \"${registryAddress}\"/authentication-salt-generator\n\n  6. docker run -it -d -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\"\n     -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-authentication -e schema_name=ida -e table_name=uin_encrypt_salt\n     \"${registryAddress}\"/authentication-salt-generator  Note  - Please change the value for variables active_profile_env, spring_config_label_env, spring_config_url_env and registryAddress in the above four commands accordingly",
            "title": "8.2 Kernel Salt Generator"
        },
        {
            "location": "/Getting-Started/#83-first-user-registration-and-onboarding",
            "text": "Refer to wiki for detailed procedure on First User Registration and Onboarding",
            "title": "8.3 First User Registration and Onboarding"
        }
    ]
}